---

**I**NTELLIGENT **C**ERVICAL **S**PINE **F**RACTURE **D**ETECTION **U**SING **D**EEP **L**EARNING **M**ETHODS

---

Reza Behbahani Nejad  
Faculty of Mechanical Engineering  
K. N. Toosi University of Technology  
Tehran, Iran bnreza982@gmail.com

Amir Hossein Komijani  
Faculty of Mechanical Engineering  
University of Tehran  
Tehran, Iran  
amir.hossein.komijani2000@gmail.com

Esmaeil Najafi  
Faculty of Smart Mechatronics And RoboTics (SMART)  
Saxion University of Applied Sciences  
M.H. Tromplaan 28 7513 AB Enschede, The Netherlands e.najafi@saxion.nl

November 13, 2023

**ABSTRACT**

Cervical spine fractures constitute a critical medical emergency, with the potential for lifelong paralysis or even fatality if left untreated or undetected. Over time, these fractures can deteriorate without intervention. To address the lack of research on the practical application of deep learning techniques for the detection of spine fractures, this study leverages a dataset containing both cervical spine fractures and non-fractured computed tomography images. This paper introduces a two-stage pipeline designed to identify the presence of cervical vertebrae in each image slice and pinpoint the location of fractures. In the first stage, a multi-input network, incorporating image and image metadata, is trained. This network is based on the Global Context Vision Transformer, and its performance is benchmarked against popular deep learning image classification models. In the second stage, a YOLOv8 model is trained to detect fractures within the images, and its effectiveness is compared to YOLOv5. The obtained results indicate that the proposed algorithm significantly reduces the workload of radiologists and enhances the accuracy of fracture detection.

**Keywords** Cervical Spine Fracture · AI in Medical Image Analysis · Deep Learning · Global Context Vision Transformer · Object Detection

## 1 Introduction

The neck is a part of the spinal column, a long, flexible structure that runs through most of the body. The cervical spine, or neck region, is made up of seven bones called vertebrae, which are separated by intervertebral discs as presented in Figure [1]. The third through the sixth cervical vertebrae show nearly identical features and are therefore considered typical of this region. The upper two cervical vertebrae, the atlas (C1), the axis (C2), and the seventh cervical vertebra (C7) are atypical. Typical Cervical Vertebrae (C3 to C6) have small rectangular bodies made of a relatively dense and strong cortical shell. The primary function of C1 is to support the head. (C2) has a large, tall body that serves as a base for the upwardly projecting dens. Vertebra Prominens (C7) is the largest of all cervical vertebrae, having many characteristics of thoracic vertebrae [1].


![Spine Cervical Vertebrae](image.png)

Figure 1: Spine Cervical Vertebraes. Adopted from [1]

Cervical Spinal Fractures (CSFx) are serious injuries that can cause death or severe disability due to damage to the spinal cord, the base of the skull where the head meets the spine, and the blood vessels in the neck. If the spinal column is unstable, it can put pressure on the spinal cord and cause further damage [2].

The rate of new spinal cord injuries has remained the same over the past decade, with 26.5 cases per 1 million people [3]. Spinal cord injuries are a major cause of disability in young adults and working people, and they have a significant impact on both individuals and society. Therefore, it is important to identify and stabilize CSFx quickly to prevent further disability. Young men are most likely to suffer spinal cord injuries, and the most common causes are traffic accidents, falls, assaults, and sports activities [4].

Initial evaluation of a patient with suspected thoracolumbar injury consists of clinical assessment including a thorough neurological examination, and diagnostic imaging. Imaging tests include traditional front-to-back and side-to-side X-rays, Computed Tomography (CT) scans, and Magnetic Resonance Imaging (MRI) scans [5]. Detecting and precisely locating vertebral fractures rapidly is imperative to prevent neurological deterioration and paralysis following traumatic incidents. Employing deep learning presents a valuable approach to achieving this objective.

Deep learning (DL) is a type of artificial intelligence (AI) that has been widely used in radiology in recent years. DL has been used to develop automatic fracture detection systems for radiographs of various body parts. With the rise of computer vision models, deep learning, and ubiquitous medical data, it is now possible to develop systems that can assist overworked medical personnel. Quicker diagnosis made possible by DL can prevent lifelong disabilities and even death in some cases. DL can also be used to analyze CT scans, which are difficult for humans to navigate due to the large number of 2D slices [6].

Several studies have explored the application of deep learning and computer vision algorithms for detecting cervical spine fractures [7, 8]. For instance, a deep convolutional neural network with a bidirectional long-short-term memory (Bi-LSTM) layer for automated fracture detection has been proposed in [9] and achieves classification accuracies of 79.18% on different datasets. The use of Vision Transformers (ViT) has been explored [10] and finds that ViT outperforms traditional Convolutional Neural Networks (CNNs) with 98% accuracy in detecting cervical spine fractures. The work [11] focuses on classifying cervical spine injuries as fractures or dislocations using deep learning models, achieving high accuracy, sensitivity, specificity, and precision values. The work [12] utilizing deep learning for automated cervical fracture detection, extensive model optimization through custom layers and data augmentation, and developing a deployable smartphone application.

Additionally, some investigations are primarily focused on the segmentation of vertebrae. They have employed techniques such as U-Net [13], either in its 2D form by processing individual slices of spine images as separate inputs to the network [14, 15] or in a 3D variant where 3D image patches from multi-slice images serve as input, and a 3D U-Net is trained [16, 17].


A PREPRINT - NOVEMBER 13, 2023

Despite the progress gap persists. While existing methodologies often focus on the segmentation of vertebrae or the detection of fractures, a more holistic approach is warranted—one that not only quantifies the number of cervical vertebrae but also ascertains whether they are fractured or intact. This comprehensive assessment is crucial for providing a thorough clinical evaluation, ensuring that no potential injuries go undetected. Moreover, the dataset employed in the training of deep learning algorithms plays a pivotal role in the accuracy and reliability of fracture detection systems. The quality, diversity, and size of the dataset significantly impact the generalizability and robustness of the algorithms, highlighting the need for standardized, high-quality medical imaging datasets. The paper is organized as follows: Section 2 reviews the relevant concepts and materials, and describes the proposed methodology. Section 3 presents the experimental results, and Section 4 discusses the results and their implications. Finally, Section 5 summarizes the main contributions of the paper and outlines future work.

2 Material and methods

This section presents an overview of the deep learning models employed in this study for the classification and detection of cervical spine fractures. The subsequent sections of this document provide an in-depth discussion of these concepts.

2.1 Convolution Neural Networks

CNNs are a type of deep learning model that are specifically designed to process data that has a grid-like topology, such as images. CNNs are typically made up of a series of layers, including convolutional layers, pooling layers, and fully connected layers.

The convolutional layer is the most important layer in a CNN architecture. It is responsible for extracting features from the input image by using a series of filters. Each filter is a small matrix of weights that is applied to a small region of the input image. The output of the convolutional layer is a feature map, which is a matrix of values that represent the features that have been extracted from the input image. The pooling layer is responsible for reducing the spatial size of the feature maps generated by the convolutional layers. This is done by applying a pooling function to each feature map. The pooling function typically takes a small region of the feature map and reduces it to a single value. The most common pooling functions are max pooling and average pooling. The fully connected layer is the final layer in a CNN architecture. It is responsible for making the final prediction, such as classifying the image or detecting objects in the image. The fully connected layer is a traditional neural network layer, meaning that each neuron in the layer is connected to every neuron in the previous layer [18,19].

CNNs offer several notable advantages in the realm of image processing. Notably, CNNs incorporate Weight Sharing, Sparse Connectivity, and Local Receptive Fields as integral design principles. Weight Sharing facilitates the sharing of weights across the spatial dimensions of the input image, reducing the number of trainable parameters, thereby enhancing efficiency and mitigating overfitting. Sparse Connectivity ensures that each neuron within a layer maintains connections with only a limited subset of neurons in the preceding layer, further bolstering efficiency and diminishing overfitting concerns. Moreover, CNNs employ local receptive fields, restricting each neuron’s responsiveness to a small, localized region of the input image, thereby enhancing robustness to noise and variations within the image. These fundamental characteristics collectively contribute to the efficacy of CNNs in image analysis and classification tasks [18,19].

Various CNN architectures have played pivotal roles in advancing computer vision tasks. VGGNet [20], ResNet [21], DenseNet [22], and ConvNeXt [23] trained on ImageNet’s extensive image collection, have consistently outperformed in image classification. Transfer learning, a technique of reusing pre-trained models on new tasks, has profoundly impacted these successes. It provides a solution for effectively training CNNs when labeled data is limited, enabling models to leverage prior knowledge acquired during the initial task. This approach not only saves time but also enhances performance by capitalizing on the learned features [19].

Overall CNNs have been shown to be very effective at a wide range of computer vision tasks, including image classification, object detection, and image segmentation.

2.2 Transformers in vision

Transformers are prominent deep learning models that have been widely adopted in various fields, such as Natural Language Processing (NLP), Computer Vision (CV), and speech processing. The Transformer architecture represents a major advancement in deep learning, relying solely on attention mechanisms rather than recurrent or convolutional layers for sequence transduction tasks [24]. The core component is the multi-head self-attention mechanism, which allows the model to capture dependencies between elements in a sequence regardless of position. Multiple parallel self-attention


A PREPRINT - NOVEMBER 13, 2023
![Figure](a_preprint_figure.png)

Figure 2: Global Context Vision Transformer network overview for vertebral classification. a. Attention formulation: Local and global attention mechanisms enable cross-region interaction. b. Global query generation: Extraction of global query tokens for long-range information. c. Global query generator: Schematic of the stage-specific dimension transformation and feature extraction process. 

heads compute different transformations of the input to model different types of relationships. Positional encodings are also a critical aspect of injecting order information into the permutation invariant self-attention. Commonly, sine and cosine functions with geometrically increasing wavelengths are used to encode position [25].

The adoption of transformers in vision comes from the vision transformer model [26]. Vision transformers apply the standard transformer architecture to image classification by converting images to sequences of patches. ViTs work by first dividing an image into a sequence of patches. These patches are then embedded into a high-dimensional space using a linear projection. The embedded patches are then fed into a transformer encoder, which learns to model the relationships between the patches. The output of the transformer encoder is then used to make predictions for the downstream task [25][27].

Although the self-attention mechanism in ViT allows for learning more uniform short and long-range information in comparison to CNN, its monolithic architecture and the quadratic computational complexity of self-attention present hurdles for its rapid implementation in the context of high-resolution images. This is a significant issue, particularly given the critical importance of accurately modeling multi-scale long-range information, as highlighted in [28].

This led to the development of several architectural variations. The Swin Transformer is a prominent variant that addresses the limitations of the original ViT, particularly its scalability to higher resolution images [29].

The Swin Transformer incorporates a range of innovative features, notably hierarchical feature maps that systematically reduce spatial dimensions, facilitating its adaptability to tasks demanding intricate details. It implements patch merging to aggregate patches into larger tokens, achieving downsampling without reliance on convolutional methods. Moreover, it adopts a window-based multi-head self-attention mechanism, concentrating computational efforts within local windows rather than across all patches for enhanced efficiency. The introduction of shifted windows promotes interactions by creating overlaps between neighboring windows, while orphaned patches are efficiently organized into incomplete windows via cyclic shifting, ensuring global connectivity[29].


Despite the advancements made, self-attention’s ability to capture long-range information is challenged by the restricted receptive field of local windows. Additionally, window-connection strategies like shifting only encompass a small surrounding area near each window.

The Global Context Vision Transformer (GCVIT) presents a pioneering vision transformer architecture aimed at optimizing parameter and computational efficiency by simultaneously accounting for local and global spatial interactions within images. GCVIT adopts a hierarchical structure characterized by the alternation of local and global self-attention modules as depicted in Figure 2.

In its design, GCVIT integrates local self-attention to capture short-range dependencies within defined local windows, a concept akin to prior models such as the Swin Transformer. However, the defining innovation in GCVIT is the introduction of global query tokens. These tokens are generated from the entire image through a CNN-like module, and they interact with local key and value tokens within each window.

This pioneering approach empowers the global self-attention mechanism to effectively capture long-range dependencies spanning across these windows. Remarkably, this is achieved without the need for computationally expensive operations like shifting windows. Furthermore, GCVIT employs a modified convolution block for downsampling, which significantly enhances the modeling of inter-channel dependencies within the network. This distinctive architecture holds immense promise for applications like vertebral classification, offering a powerful blend of local and global information capture efficiently and effectively [30].

In general, utilizing transformers in computer vision offers numerous advantages compared to traditional Convolutional Neural Networks (CNNs). ViTs excel in scalability, accommodating larger image sizes while maintaining performance, and exhibit enhanced robustness, displaying resilience to noise and occlusions. Moreover, they stand out for their ease of training, demanding less data for proficient performance. Nevertheless, ViTs come with a few drawbacks, notably the higher computational cost involved in their training and their heightened sensitivity to the size of the training dataset, which surpasses that of CNNs [25].

### 2.3 Object Detection

Object detection is a key technology in computer vision that identifies and localizes objects of interest within an image or video. The task involves both classifying the type of object present as well as determining its spatial location and extent via bounding boxes [18].

The You Only Look Once (YOLO) [31] framework has emerged as a leading approach for real-time object detection. When introduced, YOLO revolutionized object detection with its single neural network architecture that avoids region proposal and sliding window methods used in earlier approaches. The key idea behind YOLO is to divide the input image into a grid and make predictions for bounding boxes and class probabilities in one pass. This unified architecture delivers fast inference speeds while maintaining reasonable accuracy [32].

Since the original YOLO, the architecture has gone through multiple iterations of refinement from YOLOv2 to the latest YOLOv8, gradually improving the accuracy and speed. Key developments include the addition of anchor boxes, improved backbones like DarkNet, multi-scale predictions, and advanced training strategies. While early YOLO models focused on speed, later versions have balanced speed and accuracy by providing lightweight to heavy models[32].

### 2.4 Preprocessing

The dataset used in this work consists of various components, including training and test data, metadata files, and expert annotations. The ground truth dataset was created by collecting CT imaging data from twelve sites across six continents, encompassing approximately 3,000 CT studies. Radiological Society of North America (RSNA) provided expert image-level annotations, indicating the presence, vertebral level, and location of cervical spine fractures. There are target columns, such as patient overall for patient-level outcomes and C[1—7] for individual vertebrae fractures.

The primary dataset for this task is reasonably balanced, with a split of approximately 52% for non-fractured and 48% for fractured cases. Within the fractured cases, there is significant variability, with C7 having the highest proportion of fractures at 19%, while C3 exhibits the lowest incidence at 4%. It’s noteworthy that some patients may present with multiple fractures, which tend to occur in close proximity, such as between C4 and C5, rather than being dispersed across different vertebrae as shown in Figure 3. The medical image data is stored in Digital Imaging and Communications in Medicine (DICOM) format, a well-established standard for medical image storage. Information like image size, pixel dimensions, brightness, contrast, and pixel value range can be extracted from DICOM metadata, providing essential insights for image interpretation. Additionally, there are bounding boxes for a subset of the training data.


![Analysis of Fractures](image.png)

Figure 3: Distribution of fractured and non-fractured cervical vertebrae in the spine

In addition to DICOM, the dataset also includes Neuroimaging Informatics Technology Initiative (NIfTI) files, a simpler format compared to DICOM, containing the segmentation of vertebrae. However, it's crucial to align the orientation of these segmentations with the DICOM images correctly. The provided segmentations are valuable for locating vertebrae and understanding which vertebrae are present in each image. Additionally, a subset of the dataset includes bounding boxes, which specify the precise location of fractures. These bounding boxes are available for only 12% of patients in the training set, and it is suggested that training an object localization algorithm could help provide bounding boxes for the entire training set.

Overall The preprocessing phase for preparing images to be used in an image classification notebook is showcased by Algorithm 1, with selected outputs depicted in Figure 4.

| Algorithm 1: Data Preprocessing                      |
|------------------------------------------------------|
| **Data:** DICOM images and NIfTI masks               |
| **Result:** Data preprocessing and custom data generator |
| **1** | while each DICOM image in the dataset do     |
| **2** | Read the DICOM image;                        |
| **3** | Set the PhotometricInterpretation to YBRFULL;|
| **4** | Load the pixel array;                        |
| **5** | Normalize the pixel values;                  |
| **6** | Convert pixel values to a uint8 image;       |
| **7** | Convert the image to RGB format;             |
| **8** | while each NIfTI mask in the dataset do      |
| **9** | Load the NIfTI mask;                         |
| **10**| Convert the mask to a numpy array;           |
| **11**| Process the mask: flip, transpose, clip, and convert to uint8; |
| **12**| Optionally, apply further transformations to the mask; |
| **13**| Split the dataset into training and testing sets |

### 2.5 Classification of Vertebrae

This section outlines the methodology employed for cervical vertebrae classification in the current study. To achieve this objective, a multi-input deep neural network is leveraged for vertebrae classification. This network accommodates two types of inputs: images and metadata, both of which contain essential features such as image positions and slice


![Figure 4: Visualizing a selection of preprocessed dataset images](a, b, c, d)

Figure 4: Visualizing a selection of preprocessed dataset images

![Figure 5: The architecture of the proposed multi-input network.](architecture-diagram)

Figure 5: The architecture of the proposed multi-input network. Images are processed through a Global Context Vision Transformer (tiny version), while metadata is input into two fully connected layers.

ratios. Given the sequential nature of image acquisition in CT imaging, the inclusion of these metadata features proves particularly advantageous. The network architecture integrates the Global Context Vision Transformer, with pre-trained weights, to handle the image input, while the metadata input undergoes processing through three fully connected layers. Subsequently, the extracted features from both inputs are concatenated, and two additional fully connected layers are employed for the final classification. The schematic of the network is presented in Figure 5.

Furthermore, a learning rate reduction strategy is implemented through the utilization of the ReduceLROnPlateau callback. This strategy continuously monitors the validation and training F1 score and dynamically adjusts the learning rate during the training process. This adaptive learning rate strategy serves the dual purpose of enhancing model convergence and improving overall performance.

In order to assess the effectiveness of the approach employed, several deep learning models, including ResNet152V2, VGG19, DenseNet, ConvNext, Vision Transformer, and Swin Transformer, are implemented. These models operate solely on image inputs for the classification of vertebrae. The outcomes of these model implementations are summarized in Table 1.

2.6 Fracture Detection

The dataset used in this study comprises bounding boxes representing the locations of fractures on vertebrae. To enrich the dataset, an additional set of around one thousand 2D images depicting non-fractured vertebrae has been integrated. The inclusion of these non-fractured samples aims to provide a more comprehensive and balanced representation of the data, facilitating improved model training.

In the pursuit of effective fracture detection, this study leverages the capabilities of two object detection algorithms: YOLOv5 and YOLOv8. These algorithms are to identify fracture occurrences within the images. The choice of utilizing YOLOv5 and YOLOv8 is grounded in their proven effectiveness in object detection tasks, making them suitable candidates for the fracture detection objective at hand.

7


A PREPRINT - NOVEMBER 13, 2023

3 Results

The evaluation of the network employs the utilization of multiple metrics in the context of a multi-label classification task. Specifically, the evaluation metrics employed in this study encompass Macro F1, Exact Match Ratio (EMR), and Coverage Error. These metrics are calculated using the Scikit-learn library [33]. The Macro F1-score is a metric used for evaluating the performance of a multi-label classification model. It calculates the average F1 score across all the labels, providing a single value that reflects the model’s ability to simultaneously balance precision and recall for multiple classes. The F1 score is a harmonic mean of precision and recall, and it takes into account both false positives and false negatives. For each label, it measures how well the model correctly identifies true positives while minimizing false positives and false negatives. The macro F1 score then computes the mean of these label-specific F1 scores, providing a comprehensive evaluation of the model’s overall classification performance across all labels. Equation 1 depicts the formula for calculating the Macro F1 score.

\[
\text{Macro F1} = \frac{1}{N_{\text{LABELS}}} \sum_{i=1}^{N_{\text{LABELS}}} \frac{2 \cdot \text{True Positives}_i}{2 \cdot \text{True Positives}_i + \text{False Negatives}_i + \text{False Positives}_i + \epsilon}
\]

Exact Match Ratio (EMR) is another metric used to evaluate the performance of a multi-label classification model. It is calculated by the percentage of instances where the model predicts all of the correct labels for a given instance. EMR is a more stringent metric than MacroF1, as it requires the model to predict all of the correct labels for a given instance in order to be considered correct. The mathematical formula for EMR is expressed as follows in equation 2:

\[
\text{Exact Match Ratio} = \frac{\text{Number of Correctly Predicted Samples}}{\text{Total Number of Samples}} 
\]

Coverage Error is also a metric used to evaluate the performance of a multi-label classification model in terms of its ability to predict at least one of the correct labels for a given instance. It is calculated by the percentage of instances where the model predicts no correct labels for a given instance. Coverage Error is a more lenient metric than EMR, as it allows the model to be partially correct as long as it selects at least one of the correct labels. The coverage error is calculated as shown in equation 3.

\[
\text{Coverage Error} = \frac{1}{N} \sum_{i=1}^{N} (\text{Number of Additional Labels}_i)
\]

Table 1: Performance metrics of different cervical spine vertebrae classification models

| Model              | MacroF1 | Exact Match Ratio | Coverage Error | Trainable Parameters | Non-trainable Parameters |
|--------------------|---------|-------------------|----------------|----------------------|--------------------------|
| Proposed Network   | 0.96    | 0.95              | 1.26           | 13,080,663           | 14,683,998               |
| ViT                | 0.94    | 0.90              | 1.41           | 56,937,479           | 30,764,544               |
| Convext            | 0.95    | 0.93              | 1.35           | 9,866,535            | 39,966,816               |
| InceptionV3        | 0.92    | 0.89              | 1.45           | 526,535              | 21,802,592               |
| ResNet152V2        | 0.96    | 0.94              | 1.26           | 6,045,703            | 52,812,288               |
| Swin Transformer   | 0.95    | 0.909             | 1.36           | 49,899,081           | 2,768                    |

The results of the multi-label classification for cervical spine vertebrae demonstrate the performance of various neural network models. The proposed network, with a MacroF1 score of 0.96 and an Exact Match Ratio of 0.95, exhibits promising results when compared to other models, including ViT, Convext, InceptionV3, ResNet152V2, and Swin Transformer. Table 2 displays the classification report for the proposed network, illustrating its strong performance in multi-label cervical spine vertebrae classification. The network achieves high precision (0.97 to 1.00), recall (0.93 to 0.98), and F1-scores (0.95 to 0.99) across all seven classes (C1 to C7), indicating its effectiveness in correctly identifying vertebrae. The micro, macro, and weighted averages are all approximately 0.97, showing consistent overall performance. The loss diagram of the proposed network is also presented in Figure 6, revealing a consistent downward trend over the course of 25 training epochs.

8

A PREPRINT - NOVEMBER 13, 2023

Table 2: Multi-label classification report for cervical spine vertebrae using the proposed network

| Class  | Precision | Recall | F1-score | Support |
|--------|-----------|--------|----------|---------|
| C1     | 0.97      | 0.93   | 0.95     | 284     |
| C2     | 0.98      | 0.97   | 0.98     | 455     |
| C3     | 1.00      | 0.98   | 0.99     | 264     |
| C4     | 0.97      | 0.97   | 0.97     | 272     |
| C5     | 0.98      | 0.97   | 0.97     | 277     |
| C6     | 0.99      | 0.96   | 0.97     | 278     |
| C7     | 0.98      | 0.98   | 0.98     | 320     |
| Micro avg | 0.98   | 0.97   | 0.97     | 2150    |
| Macro avg | 0.98   | 0.97   | 0.97     | 2150    |
| Weighted avg | 0.98 | 0.97 | 0.97     | 2150    |

![Training and Validation Loss](images/loss_curve.png)

Figure 6: Visualization of Loss for the Multi-Input Network Employing the Global Context Vision Transformer Model in Cervical Vertebrae Classification

For the cervical vertebrae fracture detection, the models are trained 100 epochs and the results are demonstrated in Table 3.

mAP50 is the Mean Average Precision (mAP) at an IoU threshold of 0.5. IoU (Intersection over Union) is a metric used to measure the overlap between a predicted bounding box and a ground truth bounding box. A higher IoU threshold means that the predicted bounding box must overlap more with the ground truth bounding box to be considered a true detection. mAP50-95 is the Mean Average Precision (mAP) at IoU thresholds from 0.5 to 0.95. This is a more comprehensive metric than mAP50, as it takes into account the model’s performance at a wider range of IoU thresholds. Equations 4, 5, and 6 define the formulas for mAP50, mAP50-95, Recall, and Precision.

\[ \text{mAP50} = \frac{1}{N} \sum_{i=1}^{N} \text{AP}_{50}^{(i)} \]

(4)

9


A PREPRINT - NOVEMBER 13, 2023

Table 3: YOLO Performance Metrics

| Model   | Class | Images | Instances | Box(P) | R     | mAP50 | mAP50-95 |
|---------|-------|--------|-----------|--------|-------|-------|----------|
| YOLOv8s | all   | 1644   | 1444      | 0.94   | 0.921 | 0.96  | 0.747    |
|         | 1     | 1644   | 1444      | 0.94   | 0.921 | 0.96  | 0.747    |
| YOLOv8m | all   | 1644   | 1444      | 0.938  | 0.927 | 0.959 | 0.766    |
|         | 1     | 1644   | 1444      | 0.938  | 0.927 | 0.959 | 0.766    |
| YOLOv5s | all   | 1644   | 1444      | 0.935  | 0.919 | 0.95  | 0.721    |
|         | 1     | 1644   | 1444      | 0.935  | 0.919 | 0.95  | 0.721    |
| YOLOv5m | all   | 1644   | 1444      | 0.932  | 0.931 | 0.958 | 0.748    |
|         | 1     | 1644   | 1444      | 0.932  | 0.931 | 0.958 | 0.748    |

![YOLOv8s Loss Diagrams](image1.png)
![YOLOv8s Metrics Diagrams](image2.png)

Figure 7: Comprehensive Loss and Metric Analysis for YOLOv8s in Cervical Vertebrae Spine Detection.

\[ \text{mAP50-90} = \frac{1}{N} \sum_{i=1}^{N} \text{AP}_{50-90}^{(i)} \]

\[ \text{Recall (R)} = \frac{\text{True Positives}}{\text{True Positives + False Negatives}} \]

(5)

(6)

10

![YOLOv8s Loss Diagrams](<image_path>)

![YOLOv8s Metrics Diagrams](<image_path>)

Figure 8: Comprehensive Loss and Metric Analysis for YOLOv8m in Cervical Vertebrae Spine Detection.

![Confusion Matrix for YOLOv8s](<image_path>) ![Confusion Matrix for YOLOv8m](<image_path>)

Figure 9: Confusion Matrices for YOLOv8s and YOLOv8m Models in the Detection of Cervical Vertebrae Fractures

\[ 
\text{Precision (P)} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}} \quad (7)
\]

The loss and metric diagrams for YOLOv8s and YOLOv8m are also presented in Figure 7 and Figure 8 respectively showing a downward trend. A curated set of predictions, exemplifying the proficiency of the YOLOv8s model in detecting cervical spine fractures, is visually depicted in Figure 10.

11

A PREPRINT - NOVEMBER 13, 2023

![Figure 10: Visualization of YOLO Algorithm Predictions](figure10.png)

Figure 10: Visualization of YOLO Algorithm Predictions on Cervical Vertebrae Spine Fracture Detection Dataset Images. Subfigures (a)-(f) display a selection of predictions, showcasing the YOLOv8 model's performance in detecting cervical spine fractures.

4 Discussion

This paper presents a two-step pipeline aimed at detecting cervical vertebrae within individual image slices and locating fractures. In the initial stage, a multi-input network, which includes both the image data and associated image metadata, undergoes training. This network is constructed upon the Global Context Vision Transformer architecture and is evaluated in terms of performance, with a comparison to well-known deep learning image classification models. In the subsequent stage, a YOLOv8 model is trained specifically for fracture detection in the images, and its performance is assessed in relation to YOLOv5.

One of the notable strengths of the proposed network is its lower Coverage Error, which stands at 1.26. This indicates that it predicts fewer unnecessary labels compared to some of the other models, such as ViT with a Coverage Error of 1.41 and Convext with a Coverage Error of 1.35. This lower Coverage Error suggests that the proposed network produces more precise results, which is a significant advantage for this specific classification task.

On the downside, it’s important to consider the relatively high number of non-trainable parameters in the proposed network (14,683,998). High non-trainable parameter counts can lead to increased memory and computational requirements, potentially limiting its practicality in resource-constrained environments.

Overall, the proposed network demonstrates strong performance in cervical spine vertebrae classification, with a competitive MacroF1 score, high Exact Match Ratio, and a notable advantage in terms of Coverage Error. However, the high number of non-trainable parameters is a potential drawback to be addressed for certain deployment scenarios. Careful consideration of the trade-offs between precision and model complexity is crucial when determining the suitability of the proposed network for specific applications.

12


A PREPRINT - NOVEMBER 13, 2023

For fracture detection based on the results, it is clear that the performance of YOLOv8 outperforms YOLOv5. This is a significant improvement, especially considering that YOLOv8 is also faster than YOLOv5. Furthermore, although YOLOv8m has more parameters the mAP50 for YOLOv8s is a bit higher. On the other hand, YOLOv8m has higher mAP50-95. It is evident that YOLOv8s has demonstrated a strong ability to correctly classify images as “Normal” with 194 true positives and only 6 false negatives as presented in Figure 9.

However, it tends to make more errors when classifying images as “Fracture,” as indicated by 97 false positives and 1347 true positives. This can be attributed to the nature of medical image analysis, where the cost of missing a “Fracture” (false negatives) may be considerably higher than misclassifying a “Normal” image as a “Fracture” (false positives). YOLOv8m, on the other hand, demonstrates a similar trend but with slightly improved performance when compared to YOLOv8s. It correctly classifies 192 “Normal” images and 1353 “Fracture” images. However, it still makes some errors, with 8 false negatives for “Normal” and 91 false positives for “Fracture”. This model appears to strike a better balance between precision and recall for both classes, indicating a more robust classification performance.

5 Conclusion

The study introduced a two-stage pipeline leveraging deep learning models for the purpose of classifying vertebrae and detecting cervical spine fractures. The first stage incorporated a multi-input network with a Global Context Vision Transformer (GCViT) for vertebrae classification, while the second stage employed YOLOv8 for fracture detection. The outcomes are subsequently compared against existing deep learning-based image classification models, yielding noteworthy results. The proposed architecture demonstrated its efficacy, achieving a commendable 96% Macro FI accuracy for vertebrae classification and a Mean Average Precision (mAP) of 96% for fracture detection.

In terms of future research directions, it is worth exploring the potential integration of segmentation models to further enhance the precision of cervical spine fracture identification. Such segmentation models can facilitate the delineation of distinct anatomical structures within the cervical region, ultimately refining the diagnostic process.

In summary, this study focuses on investigating medical image diagnostics, specifically in the early and accurate identification of cervical spine fractures. The two-stage approach introduces significant advancements towards improving the management of critical medical injuries and alleviating the burden on radiologists.

References

[1] Donald A Neumann. Kinesiology of the musculoskeletal system-e-book: foundations for rehabilitation. Elsevier Health Sciences, 2016.

[2] Alvoter ME, ME Larson, JW Garrett, and J-PJ Yu. Diagnostic accuracy and failure mode analysis of a deep learning algorithm for the detection of cervical spine fractures. American Journal of Neuroradiology, 42(8):1550–1556, 2021.

[3] Claudio Barbetelli Amidei, Laura Salmaso, Stefania Bellio, and Mario Saia. Epidemiology of traumatic spinal cord injury: a large population-based study. Spinal Cord, 60(9):812–819, 2022.

[4] Christian Zanza, Gilda Tomatore, Cristina Naturale, Yaroslava Longhitano, Angela Saviano, Andrea Piccioni, Aniello Maiese, Michela Ferrara, Gianpietro Volonnino, Giuseppe Bertozzi, et al. Cervical spine injury: Clinical and medico-legal overview. La radiologia medica, 128(1):103–112, 2023.

[5] Fabio Galbusera. Biomechanics of the spine. In Human Orthopaedic Biomechanics, pages 265–283. Elsevier, 2022.

[6] Emmanuel Montagron, Milena Cerny, Alexandre Cadrin-Chênevert, Vincent Hamilton, Thomas Derennes, André Ilinca, Franck Vandenbroucke-Men, Simon Turcotte, Samuel Kadoury, and An Tang. Deep learning workflow in radiology: a primer. Insights into imaging, 11:1–15, 2020.

[7] Yizhi Chen, Yunhe Gao, Kang Li, Liang Zhao, and Jun Zhao. Vertebrae identification and localization utilizing fully convolutional networks and a hidden markov model. IEEE Transactions on Medical Imaging, 39(2):387–399, 2019.

[8] Daniel Forsberg, Erik Sjölblom, and Jeffrey L Sunshine. Detection and labeling of vertebrae in mr images using deep learning with clinical annotations as training data. Journal of digital imaging, 30(4):406–412, 2017.

[9] Hojjat Salehinejad, Edward Ho, Hui-Ming Lin, Priscila Crivellaro, Oleksandra Samorodova, Monica Tafur Arciniegas, Zamir Merali, Suradech Suthiphosuwan, Aditya Bharatha, Kristen Yeom, et al. Deep sequential learning for cervical spine fracture detection on computed tomography imaging. In 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), pages 1911–1914. IEEE, 2021.

13


[10] Paweł Chłqał and Marek R Ogiela. Deep learning and cloud-based computation for cervical spine fracture detection system. Electronics, 12(9):2056, 2023.

[11] Soad M Naqibua, Hanaa M Hamza, Khalid M Hosny, Mohammad K Saleh, and Mohamed A Kassem. Classification of cervical spine fracture and dislocation using refined pre-trained deep model and saliency map. Diagnostics, 13(7):1407, 2023.

[12] Showmik Guha Paul, Arpa Saha, and Md Assaduzzaman. A real-time deep learning approach for classifying cervical spine fractures. Healthcare Analytics, page 100263, 2023.

[13] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234–241. Springer, 2015.

[14] Young Jae Kim, Bilegt Gangbold, and Kwang Gi Kim. Web-based spine segmentation using deep learning in computed tomography images. Healthcare informatics research, 26(1):61–67, 2020.

[15] Yijie Fang, Wei Li, Xiaojun Chen, Keming Chen, Han Kang, Pengxin Yu, Rongguo Zhang, Jianwei Liao, Guobin Hong, and Shaolin Li. Opportunistic osteoporosis screening in multi-detector ct images using deep convolutional neural networks. European Radiology, 31:1831–1842, 2021.

[16] Nikolas Lessmann, Bram Van Ginneken, Pim A De Jong, and Ivana Iˇsgum. Iterative fully convolutional neural networks for automatic vertebra segmentation and identification. Medical image analysis, 53:142–155, 2019.

[17] Guoxin Fan, Huaqing Liu, Zhenhua Wu, Yufeng Li, Chaobo Feng, Dongdong Wang, Jie Luo, WM Wells, and Shisheng He. Deep learning–based automatic segmentation of lumbosacral nerves on ct for spinal intervention: a translational study. American Journal of Neuroradiology, 40(6):1074–1081, 2019.

[18] Latiin Alzubaidi, Jinglan Zhang, Amjad J Humaidi, Ayad Al-Dujaili, Ye Duan, Omran Al-Shamma, Jos´e Santamaria, Mohammed A Fadhel, Muthana Al-Amidie, and Laith Farhan. Review of deep learning: Concepts, cnn architectures, challenges, applications, future directions. Journal of big Data, 8:1–74, 2021.

[19] Dulari Bhatt, Chirag Patel, Hardik Talsania, Jigar Patel, Rasmika Vaghela, Sharmil Pandya, Kirit Modi, and Sarman Jethva. Cnn variants for computer vision: A survey. Artificial Intelligence Review, 54(10):8973–9049, 2021.

[20] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.

[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.

[22] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700–4708, 2017.

[23] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11976–11986, 2022.

[24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

[25] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in vision: A survey. ACM computing surveys (CSUR), 54(10s):1–41, 2022.

[26] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.

[27] Yang Liu, Yao Zhang, Yixin Wang, Feng Hou, Jin Yuan, Jiang Tian, Yang Zhang, Zhongchao Shi, Jianping Fan, and Zhiqiang He. A survey of visual transformers. IEEE Transactions on Neural Networks and Learning Systems, 2023.

[28] Huanrui Yang, Hongxu Yin, Pavlo Molchanov, Hai Li, and J an Kautz. Vitresh: Vision transformer compression and parameter redistribution. 2021.

[29] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021.

[30] Ali Hatamizadeh, Hongxu Yin, Greg Heinrich, Jan Kautz, and Pavlo Molchanov. Global context vision transformers. In International Conference on Machine Learning, pages 12633–12646. PMLR, 2023.


A PREPRINT - NOVEMBER 13, 2023

[31] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object 
     detection. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 779–788, 
     2016.
     
[32] Juan Terven and Diana Cordova-Esparza. A comprehensive review of yolo: From yolov1 to yolov8 and beyond. 
     *arXiv preprint arXiv:2304.00501*, 2023.
     
[33] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, 
     V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: 
     Machine learning in Python. *Journal of Machine Learning Research*, 12:2825–2830, 2011.
