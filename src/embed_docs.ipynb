{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib, urllib.request\n",
    "from urllib.parse import urlencode, quote\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from documents import ChunkType, Document\n",
    "\n",
    "namespace = \"{http://www.w3.org/2005/Atom}\"\n",
    "\n",
    "\n",
    "arxiv_api = \"http://export.arxiv.org/api/query?search_query=cat:{category}&start={start}&max_results={num_results}&sortBy=lastUpdatedDate&sortOrder=descending\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_entry(entry: ET.Element):\n",
    "  summary_text = entry.find(f\"{namespace}summary\").text\n",
    "  title_text = entry.find(f\"{namespace}title\").text.replace('\\n','')\n",
    "  published_on = entry.find(f\"{namespace}published\").text\n",
    "\n",
    "  for ele in entry.findall(f\"{namespace}link\"):\n",
    "    if ele.attrib.get(\"title\") == \"pdf\":\n",
    "      pdf_url = ele.get(\"href\")\n",
    "      break\n",
    "  \n",
    "  return title_text, summary_text, published_on, pdf_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles(category: str, num_results=20, batch=10) -> list[Document]:\n",
    "\n",
    "  documents = []\n",
    "\n",
    "  for i in range(0, num_results, batch):\n",
    "    url = arxiv_api.format(category=category, start=i, num_results=batch)\n",
    "    data = urllib.request.urlopen(url)\n",
    "    file_string = data.read().decode('utf-8')\n",
    "    root = ET.fromstring(file_string)\n",
    "\n",
    "    entries=root.findall(f\"{namespace}entry\")\n",
    "    for i, entry_tag in enumerate(entries):\n",
    "      title, summary, published_on, pdf_url = parse_entry(entry_tag)\n",
    "      document = Document(\n",
    "        id=uuid.uuid4(),\n",
    "        category=category,\n",
    "        title=title,\n",
    "        summary=summary,\n",
    "        published_on=published_on,\n",
    "        pdf_url=pdf_url\n",
    "      )\n",
    "      documents.append(document)\n",
    "\n",
    "  return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q-fin\n",
      "cs\n",
      "stat\n",
      "eess\n"
     ]
    }
   ],
   "source": [
    "with open(\"arxiv_taxonomy.json\") as f:\n",
    "  arxiv_taxonomy: dict[str, list[str]] = json.load(f)\n",
    "\n",
    "docs: list[Document] = []\n",
    "\n",
    "for section, item in arxiv_taxonomy.items():\n",
    "  categories = item\n",
    "  print(section)\n",
    "  for category in categories:\n",
    "    docs.extend(\n",
    "      get_articles(category, num_results=100, batch=50)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: Alibaba-NLP/gte-large-en-v1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alialh/Documents/academic-gpt/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "INFO:chromadb.api.segment:Collection default is not created.\n"
     ]
    }
   ],
   "source": [
    "from chroma import ChromaDB\n",
    "\n",
    "chroma = ChromaDB(embedding_model=\"hf\", hf_model=\"Alibaba-NLP/gte-large-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Embeddings Result: (3200, 1024)\n"
     ]
    }
   ],
   "source": [
    "chroma.add(\n",
    "  [doc.to_chunk_type() for doc in docs]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
