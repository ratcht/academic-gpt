{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will simulate user entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"design me an edge detection algorithm that is accurate enough to make fractures in x-rays clearer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm import LLM\n",
    "\n",
    "llm = LLM()\n",
    "\n",
    "search_query = llm.template_query(\"user_to_search\", user_query=user_query)\n",
    "\n",
    "print(search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader import ArxivLoader, arxiv\n",
    "\n",
    "arxiv_loader = ArxivLoader()\n",
    "\n",
    "docs = list(arxiv_loader.search(search_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rerank import Reranker\n",
    "\n",
    "reranker = Reranker()\n",
    "\n",
    "reranked_indices = reranker.rerank(user_query, docs)\n",
    "\n",
    "reranked_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "  print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from documents import DocumentFormat\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = await asyncio.gather(DocumentFormat.load_document_pdf(pdf_url=\"http://arxiv.org/pdf/2311.05708v1\"), DocumentFormat.load_document_pdf(pdf_url=\"https://arxiv.org/pdf/1902.07458v1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"---\\n\\n**I**NTELLIGENT **C**ERVICAL **S**PINE **F**RACTURE **D**ETECTION **U**SING **D**EEP **L**EARNING **M**ETHODS\\n\\n---\\n\\nReza Behbahani Nejad  \\nFaculty of Mechanical Engineering  \\nK. N. Toosi University of Technology  \\nTehran, Iran bnreza982@gmail.com\\n\\nAmir Hossein Komijani  \\nFaculty of Mechanical Engineering  \\nUniversity of Tehran  \\nTehran, Iran  \\namir.hossein.komijani2000@gmail.com\\n\\nEsmaeil Najafi  \\nFaculty of Smart Mechatronics And RoboTics (SMART)  \\nSaxion University of Applied Sciences  \\nM.H. Tromplaan 28 7513 AB Enschede, The Netherlands e.najafi@saxion.nl\\n\\nNovember 13, 2023\\n\\n**ABSTRACT**\\n\\nCervical spine fractures constitute a critical medical emergency, with the potential for lifelong paralysis or even fatality if left untreated or undetected. Over time, these fractures can deteriorate without intervention. To address the lack of research on the practical application of deep learning techniques for the detection of spine fractures, this study leverages a dataset containing both cervical spine fractures and non-fractured computed tomography images. This paper introduces a two-stage pipeline designed to identify the presence of cervical vertebrae in each image slice and pinpoint the location of fractures. In the first stage, a multi-input network, incorporating image and image metadata, is trained. This network is based on the Global Context Vision Transformer, and its performance is benchmarked against popular deep learning image classification models. In the second stage, a YOLOv8 model is trained to detect fractures within the images, and its effectiveness is compared to YOLOv5. The obtained results indicate that the proposed algorithm significantly reduces the workload of radiologists and enhances the accuracy of fracture detection.\\n\\n**Keywords** Cervical Spine Fracture · AI in Medical Image Analysis · Deep Learning · Global Context Vision Transformer · Object Detection\\n\\n## 1 Introduction\\n\\nThe neck is a part of the spinal column, a long, flexible structure that runs through most of the body. The cervical spine, or neck region, is made up of seven bones called vertebrae, which are separated by intervertebral discs as presented in Figure [1]. The third through the sixth cervical vertebrae show nearly identical features and are therefore considered typical of this region. The upper two cervical vertebrae, the atlas (C1), the axis (C2), and the seventh cervical vertebra (C7) are atypical. Typical Cervical Vertebrae (C3 to C6) have small rectangular bodies made of a relatively dense and strong cortical shell. The primary function of C1 is to support the head. (C2) has a large, tall body that serves as a base for the upwardly projecting dens. Vertebra Prominens (C7) is the largest of all cervical vertebrae, having many characteristics of thoracic vertebrae [1].\\n![Spine Cervical Vertebrae](image.png)\\n\\nFigure 1: Spine Cervical Vertebraes. Adopted from [1]\\n\\nCervical Spinal Fractures (CSFx) are serious injuries that can cause death or severe disability due to damage to the spinal cord, the base of the skull where the head meets the spine, and the blood vessels in the neck. If the spinal column is unstable, it can put pressure on the spinal cord and cause further damage [2].\\n\\nThe rate of new spinal cord injuries has remained the same over the past decade, with 26.5 cases per 1 million people [3]. Spinal cord injuries are a major cause of disability in young adults and working people, and they have a significant impact on both individuals and society. Therefore, it is important to identify and stabilize CSFx quickly to prevent further disability. Young men are most likely to suffer spinal cord injuries, and the most common causes are traffic accidents, falls, assaults, and sports activities [4].\\n\\nInitial evaluation of a patient with suspected thoracolumbar injury consists of clinical assessment including a thorough neurological examination, and diagnostic imaging. Imaging tests include traditional front-to-back and side-to-side X-rays, Computed Tomography (CT) scans, and Magnetic Resonance Imaging (MRI) scans [5]. Detecting and precisely locating vertebral fractures rapidly is imperative to prevent neurological deterioration and paralysis following traumatic incidents. Employing deep learning presents a valuable approach to achieving this objective.\\n\\nDeep learning (DL) is a type of artificial intelligence (AI) that has been widely used in radiology in recent years. DL has been used to develop automatic fracture detection systems for radiographs of various body parts. With the rise of computer vision models, deep learning, and ubiquitous medical data, it is now possible to develop systems that can assist overworked medical personnel. Quicker diagnosis made possible by DL can prevent lifelong disabilities and even death in some cases. DL can also be used to analyze CT scans, which are difficult for humans to navigate due to the large number of 2D slices [6].\\n\\nSeveral studies have explored the application of deep learning and computer vision algorithms for detecting cervical spine fractures [7, 8]. For instance, a deep convolutional neural network with a bidirectional long-short-term memory (Bi-LSTM) layer for automated fracture detection has been proposed in [9] and achieves classification accuracies of 79.18% on different datasets. The use of Vision Transformers (ViT) has been explored [10] and finds that ViT outperforms traditional Convolutional Neural Networks (CNNs) with 98% accuracy in detecting cervical spine fractures. The work [11] focuses on classifying cervical spine injuries as fractures or dislocations using deep learning models, achieving high accuracy, sensitivity, specificity, and precision values. The work [12] utilizing deep learning for automated cervical fracture detection, extensive model optimization through custom layers and data augmentation, and developing a deployable smartphone application.\\n\\nAdditionally, some investigations are primarily focused on the segmentation of vertebrae. They have employed techniques such as U-Net [13], either in its 2D form by processing individual slices of spine images as separate inputs to the network [14, 15] or in a 3D variant where 3D image patches from multi-slice images serve as input, and a 3D U-Net is trained [16, 17].\\nA PREPRINT - NOVEMBER 13, 2023\\n\\nDespite the progress gap persists. While existing methodologies often focus on the segmentation of vertebrae or the detection of fractures, a more holistic approach is warranted—one that not only quantifies the number of cervical vertebrae but also ascertains whether they are fractured or intact. This comprehensive assessment is crucial for providing a thorough clinical evaluation, ensuring that no potential injuries go undetected. Moreover, the dataset employed in the training of deep learning algorithms plays a pivotal role in the accuracy and reliability of fracture detection systems. The quality, diversity, and size of the dataset significantly impact the generalizability and robustness of the algorithms, highlighting the need for standardized, high-quality medical imaging datasets. The paper is organized as follows: Section 2 reviews the relevant concepts and materials, and describes the proposed methodology. Section 3 presents the experimental results, and Section 4 discusses the results and their implications. Finally, Section 5 summarizes the main contributions of the paper and outlines future work.\\n\\n2 Material and methods\\n\\nThis section presents an overview of the deep learning models employed in this study for the classification and detection of cervical spine fractures. The subsequent sections of this document provide an in-depth discussion of these concepts.\\n\\n2.1 Convolution Neural Networks\\n\\nCNNs are a type of deep learning model that are specifically designed to process data that has a grid-like topology, such as images. CNNs are typically made up of a series of layers, including convolutional layers, pooling layers, and fully connected layers.\\n\\nThe convolutional layer is the most important layer in a CNN architecture. It is responsible for extracting features from the input image by using a series of filters. Each filter is a small matrix of weights that is applied to a small region of the input image. The output of the convolutional layer is a feature map, which is a matrix of values that represent the features that have been extracted from the input image. The pooling layer is responsible for reducing the spatial size of the feature maps generated by the convolutional layers. This is done by applying a pooling function to each feature map. The pooling function typically takes a small region of the feature map and reduces it to a single value. The most common pooling functions are max pooling and average pooling. The fully connected layer is the final layer in a CNN architecture. It is responsible for making the final prediction, such as classifying the image or detecting objects in the image. The fully connected layer is a traditional neural network layer, meaning that each neuron in the layer is connected to every neuron in the previous layer [18,19].\\n\\nCNNs offer several notable advantages in the realm of image processing. Notably, CNNs incorporate Weight Sharing, Sparse Connectivity, and Local Receptive Fields as integral design principles. Weight Sharing facilitates the sharing of weights across the spatial dimensions of the input image, reducing the number of trainable parameters, thereby enhancing efficiency and mitigating overfitting. Sparse Connectivity ensures that each neuron within a layer maintains connections with only a limited subset of neurons in the preceding layer, further bolstering efficiency and diminishing overfitting concerns. Moreover, CNNs employ local receptive fields, restricting each neuron’s responsiveness to a small, localized region of the input image, thereby enhancing robustness to noise and variations within the image. These fundamental characteristics collectively contribute to the efficacy of CNNs in image analysis and classification tasks [18,19].\\n\\nVarious CNN architectures have played pivotal roles in advancing computer vision tasks. VGGNet [20], ResNet [21], DenseNet [22], and ConvNeXt [23] trained on ImageNet’s extensive image collection, have consistently outperformed in image classification. Transfer learning, a technique of reusing pre-trained models on new tasks, has profoundly impacted these successes. It provides a solution for effectively training CNNs when labeled data is limited, enabling models to leverage prior knowledge acquired during the initial task. This approach not only saves time but also enhances performance by capitalizing on the learned features [19].\\n\\nOverall CNNs have been shown to be very effective at a wide range of computer vision tasks, including image classification, object detection, and image segmentation.\\n\\n2.2 Transformers in vision\\n\\nTransformers are prominent deep learning models that have been widely adopted in various fields, such as Natural Language Processing (NLP), Computer Vision (CV), and speech processing. The Transformer architecture represents a major advancement in deep learning, relying solely on attention mechanisms rather than recurrent or convolutional layers for sequence transduction tasks [24]. The core component is the multi-head self-attention mechanism, which allows the model to capture dependencies between elements in a sequence regardless of position. Multiple parallel self-attention\\nA PREPRINT - NOVEMBER 13, 2023\\n![Figure](a_preprint_figure.png)\\n\\nFigure 2: Global Context Vision Transformer network overview for vertebral classification. a. Attention formulation: Local and global attention mechanisms enable cross-region interaction. b. Global query generation: Extraction of global query tokens for long-range information. c. Global query generator: Schematic of the stage-specific dimension transformation and feature extraction process. \\n\\nheads compute different transformations of the input to model different types of relationships. Positional encodings are also a critical aspect of injecting order information into the permutation invariant self-attention. Commonly, sine and cosine functions with geometrically increasing wavelengths are used to encode position [25].\\n\\nThe adoption of transformers in vision comes from the vision transformer model [26]. Vision transformers apply the standard transformer architecture to image classification by converting images to sequences of patches. ViTs work by first dividing an image into a sequence of patches. These patches are then embedded into a high-dimensional space using a linear projection. The embedded patches are then fed into a transformer encoder, which learns to model the relationships between the patches. The output of the transformer encoder is then used to make predictions for the downstream task [25][27].\\n\\nAlthough the self-attention mechanism in ViT allows for learning more uniform short and long-range information in comparison to CNN, its monolithic architecture and the quadratic computational complexity of self-attention present hurdles for its rapid implementation in the context of high-resolution images. This is a significant issue, particularly given the critical importance of accurately modeling multi-scale long-range information, as highlighted in [28].\\n\\nThis led to the development of several architectural variations. The Swin Transformer is a prominent variant that addresses the limitations of the original ViT, particularly its scalability to higher resolution images [29].\\n\\nThe Swin Transformer incorporates a range of innovative features, notably hierarchical feature maps that systematically reduce spatial dimensions, facilitating its adaptability to tasks demanding intricate details. It implements patch merging to aggregate patches into larger tokens, achieving downsampling without reliance on convolutional methods. Moreover, it adopts a window-based multi-head self-attention mechanism, concentrating computational efforts within local windows rather than across all patches for enhanced efficiency. The introduction of shifted windows promotes interactions by creating overlaps between neighboring windows, while orphaned patches are efficiently organized into incomplete windows via cyclic shifting, ensuring global connectivity[29].\\nDespite the advancements made, self-attention’s ability to capture long-range information is challenged by the restricted receptive field of local windows. Additionally, window-connection strategies like shifting only encompass a small surrounding area near each window.\\n\\nThe Global Context Vision Transformer (GCVIT) presents a pioneering vision transformer architecture aimed at optimizing parameter and computational efficiency by simultaneously accounting for local and global spatial interactions within images. GCVIT adopts a hierarchical structure characterized by the alternation of local and global self-attention modules as depicted in Figure 2.\\n\\nIn its design, GCVIT integrates local self-attention to capture short-range dependencies within defined local windows, a concept akin to prior models such as the Swin Transformer. However, the defining innovation in GCVIT is the introduction of global query tokens. These tokens are generated from the entire image through a CNN-like module, and they interact with local key and value tokens within each window.\\n\\nThis pioneering approach empowers the global self-attention mechanism to effectively capture long-range dependencies spanning across these windows. Remarkably, this is achieved without the need for computationally expensive operations like shifting windows. Furthermore, GCVIT employs a modified convolution block for downsampling, which significantly enhances the modeling of inter-channel dependencies within the network. This distinctive architecture holds immense promise for applications like vertebral classification, offering a powerful blend of local and global information capture efficiently and effectively [30].\\n\\nIn general, utilizing transformers in computer vision offers numerous advantages compared to traditional Convolutional Neural Networks (CNNs). ViTs excel in scalability, accommodating larger image sizes while maintaining performance, and exhibit enhanced robustness, displaying resilience to noise and occlusions. Moreover, they stand out for their ease of training, demanding less data for proficient performance. Nevertheless, ViTs come with a few drawbacks, notably the higher computational cost involved in their training and their heightened sensitivity to the size of the training dataset, which surpasses that of CNNs [25].\\n\\n### 2.3 Object Detection\\n\\nObject detection is a key technology in computer vision that identifies and localizes objects of interest within an image or video. The task involves both classifying the type of object present as well as determining its spatial location and extent via bounding boxes [18].\\n\\nThe You Only Look Once (YOLO) [31] framework has emerged as a leading approach for real-time object detection. When introduced, YOLO revolutionized object detection with its single neural network architecture that avoids region proposal and sliding window methods used in earlier approaches. The key idea behind YOLO is to divide the input image into a grid and make predictions for bounding boxes and class probabilities in one pass. This unified architecture delivers fast inference speeds while maintaining reasonable accuracy [32].\\n\\nSince the original YOLO, the architecture has gone through multiple iterations of refinement from YOLOv2 to the latest YOLOv8, gradually improving the accuracy and speed. Key developments include the addition of anchor boxes, improved backbones like DarkNet, multi-scale predictions, and advanced training strategies. While early YOLO models focused on speed, later versions have balanced speed and accuracy by providing lightweight to heavy models[32].\\n\\n### 2.4 Preprocessing\\n\\nThe dataset used in this work consists of various components, including training and test data, metadata files, and expert annotations. The ground truth dataset was created by collecting CT imaging data from twelve sites across six continents, encompassing approximately 3,000 CT studies. Radiological Society of North America (RSNA) provided expert image-level annotations, indicating the presence, vertebral level, and location of cervical spine fractures. There are target columns, such as patient overall for patient-level outcomes and C[1—7] for individual vertebrae fractures.\\n\\nThe primary dataset for this task is reasonably balanced, with a split of approximately 52% for non-fractured and 48% for fractured cases. Within the fractured cases, there is significant variability, with C7 having the highest proportion of fractures at 19%, while C3 exhibits the lowest incidence at 4%. It’s noteworthy that some patients may present with multiple fractures, which tend to occur in close proximity, such as between C4 and C5, rather than being dispersed across different vertebrae as shown in Figure 3. The medical image data is stored in Digital Imaging and Communications in Medicine (DICOM) format, a well-established standard for medical image storage. Information like image size, pixel dimensions, brightness, contrast, and pixel value range can be extracted from DICOM metadata, providing essential insights for image interpretation. Additionally, there are bounding boxes for a subset of the training data.\\n![Analysis of Fractures](image.png)\\n\\nFigure 3: Distribution of fractured and non-fractured cervical vertebrae in the spine\\n\\nIn addition to DICOM, the dataset also includes Neuroimaging Informatics Technology Initiative (NIfTI) files, a simpler format compared to DICOM, containing the segmentation of vertebrae. However, it's crucial to align the orientation of these segmentations with the DICOM images correctly. The provided segmentations are valuable for locating vertebrae and understanding which vertebrae are present in each image. Additionally, a subset of the dataset includes bounding boxes, which specify the precise location of fractures. These bounding boxes are available for only 12% of patients in the training set, and it is suggested that training an object localization algorithm could help provide bounding boxes for the entire training set.\\n\\nOverall The preprocessing phase for preparing images to be used in an image classification notebook is showcased by Algorithm 1, with selected outputs depicted in Figure 4.\\n\\n| Algorithm 1: Data Preprocessing                      |\\n|------------------------------------------------------|\\n| **Data:** DICOM images and NIfTI masks               |\\n| **Result:** Data preprocessing and custom data generator |\\n| **1** | while each DICOM image in the dataset do     |\\n| **2** | Read the DICOM image;                        |\\n| **3** | Set the PhotometricInterpretation to YBRFULL;|\\n| **4** | Load the pixel array;                        |\\n| **5** | Normalize the pixel values;                  |\\n| **6** | Convert pixel values to a uint8 image;       |\\n| **7** | Convert the image to RGB format;             |\\n| **8** | while each NIfTI mask in the dataset do      |\\n| **9** | Load the NIfTI mask;                         |\\n| **10**| Convert the mask to a numpy array;           |\\n| **11**| Process the mask: flip, transpose, clip, and convert to uint8; |\\n| **12**| Optionally, apply further transformations to the mask; |\\n| **13**| Split the dataset into training and testing sets |\\n\\n### 2.5 Classification of Vertebrae\\n\\nThis section outlines the methodology employed for cervical vertebrae classification in the current study. To achieve this objective, a multi-input deep neural network is leveraged for vertebrae classification. This network accommodates two types of inputs: images and metadata, both of which contain essential features such as image positions and slice\\n![Figure 4: Visualizing a selection of preprocessed dataset images](a, b, c, d)\\n\\nFigure 4: Visualizing a selection of preprocessed dataset images\\n\\n![Figure 5: The architecture of the proposed multi-input network.](architecture-diagram)\\n\\nFigure 5: The architecture of the proposed multi-input network. Images are processed through a Global Context Vision Transformer (tiny version), while metadata is input into two fully connected layers.\\n\\nratios. Given the sequential nature of image acquisition in CT imaging, the inclusion of these metadata features proves particularly advantageous. The network architecture integrates the Global Context Vision Transformer, with pre-trained weights, to handle the image input, while the metadata input undergoes processing through three fully connected layers. Subsequently, the extracted features from both inputs are concatenated, and two additional fully connected layers are employed for the final classification. The schematic of the network is presented in Figure 5.\\n\\nFurthermore, a learning rate reduction strategy is implemented through the utilization of the ReduceLROnPlateau callback. This strategy continuously monitors the validation and training F1 score and dynamically adjusts the learning rate during the training process. This adaptive learning rate strategy serves the dual purpose of enhancing model convergence and improving overall performance.\\n\\nIn order to assess the effectiveness of the approach employed, several deep learning models, including ResNet152V2, VGG19, DenseNet, ConvNext, Vision Transformer, and Swin Transformer, are implemented. These models operate solely on image inputs for the classification of vertebrae. The outcomes of these model implementations are summarized in Table 1.\\n\\n2.6 Fracture Detection\\n\\nThe dataset used in this study comprises bounding boxes representing the locations of fractures on vertebrae. To enrich the dataset, an additional set of around one thousand 2D images depicting non-fractured vertebrae has been integrated. The inclusion of these non-fractured samples aims to provide a more comprehensive and balanced representation of the data, facilitating improved model training.\\n\\nIn the pursuit of effective fracture detection, this study leverages the capabilities of two object detection algorithms: YOLOv5 and YOLOv8. These algorithms are to identify fracture occurrences within the images. The choice of utilizing YOLOv5 and YOLOv8 is grounded in their proven effectiveness in object detection tasks, making them suitable candidates for the fracture detection objective at hand.\\n\\n7\\nA PREPRINT - NOVEMBER 13, 2023\\n\\n3 Results\\n\\nThe evaluation of the network employs the utilization of multiple metrics in the context of a multi-label classification task. Specifically, the evaluation metrics employed in this study encompass Macro F1, Exact Match Ratio (EMR), and Coverage Error. These metrics are calculated using the Scikit-learn library [33]. The Macro F1-score is a metric used for evaluating the performance of a multi-label classification model. It calculates the average F1 score across all the labels, providing a single value that reflects the model’s ability to simultaneously balance precision and recall for multiple classes. The F1 score is a harmonic mean of precision and recall, and it takes into account both false positives and false negatives. For each label, it measures how well the model correctly identifies true positives while minimizing false positives and false negatives. The macro F1 score then computes the mean of these label-specific F1 scores, providing a comprehensive evaluation of the model’s overall classification performance across all labels. Equation 1 depicts the formula for calculating the Macro F1 score.\\n\\n\\\\[\\n\\\\text{Macro F1} = \\\\frac{1}{N_{\\\\text{LABELS}}} \\\\sum_{i=1}^{N_{\\\\text{LABELS}}} \\\\frac{2 \\\\cdot \\\\text{True Positives}_i}{2 \\\\cdot \\\\text{True Positives}_i + \\\\text{False Negatives}_i + \\\\text{False Positives}_i + \\\\epsilon}\\n\\\\]\\n\\nExact Match Ratio (EMR) is another metric used to evaluate the performance of a multi-label classification model. It is calculated by the percentage of instances where the model predicts all of the correct labels for a given instance. EMR is a more stringent metric than MacroF1, as it requires the model to predict all of the correct labels for a given instance in order to be considered correct. The mathematical formula for EMR is expressed as follows in equation 2:\\n\\n\\\\[\\n\\\\text{Exact Match Ratio} = \\\\frac{\\\\text{Number of Correctly Predicted Samples}}{\\\\text{Total Number of Samples}} \\n\\\\]\\n\\nCoverage Error is also a metric used to evaluate the performance of a multi-label classification model in terms of its ability to predict at least one of the correct labels for a given instance. It is calculated by the percentage of instances where the model predicts no correct labels for a given instance. Coverage Error is a more lenient metric than EMR, as it allows the model to be partially correct as long as it selects at least one of the correct labels. The coverage error is calculated as shown in equation 3.\\n\\n\\\\[\\n\\\\text{Coverage Error} = \\\\frac{1}{N} \\\\sum_{i=1}^{N} (\\\\text{Number of Additional Labels}_i)\\n\\\\]\\n\\nTable 1: Performance metrics of different cervical spine vertebrae classification models\\n\\n| Model              | MacroF1 | Exact Match Ratio | Coverage Error | Trainable Parameters | Non-trainable Parameters |\\n|--------------------|---------|-------------------|----------------|----------------------|--------------------------|\\n| Proposed Network   | 0.96    | 0.95              | 1.26           | 13,080,663           | 14,683,998               |\\n| ViT                | 0.94    | 0.90              | 1.41           | 56,937,479           | 30,764,544               |\\n| Convext            | 0.95    | 0.93              | 1.35           | 9,866,535            | 39,966,816               |\\n| InceptionV3        | 0.92    | 0.89              | 1.45           | 526,535              | 21,802,592               |\\n| ResNet152V2        | 0.96    | 0.94              | 1.26           | 6,045,703            | 52,812,288               |\\n| Swin Transformer   | 0.95    | 0.909             | 1.36           | 49,899,081           | 2,768                    |\\n\\nThe results of the multi-label classification for cervical spine vertebrae demonstrate the performance of various neural network models. The proposed network, with a MacroF1 score of 0.96 and an Exact Match Ratio of 0.95, exhibits promising results when compared to other models, including ViT, Convext, InceptionV3, ResNet152V2, and Swin Transformer. Table 2 displays the classification report for the proposed network, illustrating its strong performance in multi-label cervical spine vertebrae classification. The network achieves high precision (0.97 to 1.00), recall (0.93 to 0.98), and F1-scores (0.95 to 0.99) across all seven classes (C1 to C7), indicating its effectiveness in correctly identifying vertebrae. The micro, macro, and weighted averages are all approximately 0.97, showing consistent overall performance. The loss diagram of the proposed network is also presented in Figure 6, revealing a consistent downward trend over the course of 25 training epochs.\\n\\n8A PREPRINT - NOVEMBER 13, 2023\\n\\nTable 2: Multi-label classification report for cervical spine vertebrae using the proposed network\\n\\n| Class  | Precision | Recall | F1-score | Support |\\n|--------|-----------|--------|----------|---------|\\n| C1     | 0.97      | 0.93   | 0.95     | 284     |\\n| C2     | 0.98      | 0.97   | 0.98     | 455     |\\n| C3     | 1.00      | 0.98   | 0.99     | 264     |\\n| C4     | 0.97      | 0.97   | 0.97     | 272     |\\n| C5     | 0.98      | 0.97   | 0.97     | 277     |\\n| C6     | 0.99      | 0.96   | 0.97     | 278     |\\n| C7     | 0.98      | 0.98   | 0.98     | 320     |\\n| Micro avg | 0.98   | 0.97   | 0.97     | 2150    |\\n| Macro avg | 0.98   | 0.97   | 0.97     | 2150    |\\n| Weighted avg | 0.98 | 0.97 | 0.97     | 2150    |\\n\\n![Training and Validation Loss](images/loss_curve.png)\\n\\nFigure 6: Visualization of Loss for the Multi-Input Network Employing the Global Context Vision Transformer Model in Cervical Vertebrae Classification\\n\\nFor the cervical vertebrae fracture detection, the models are trained 100 epochs and the results are demonstrated in Table 3.\\n\\nmAP50 is the Mean Average Precision (mAP) at an IoU threshold of 0.5. IoU (Intersection over Union) is a metric used to measure the overlap between a predicted bounding box and a ground truth bounding box. A higher IoU threshold means that the predicted bounding box must overlap more with the ground truth bounding box to be considered a true detection. mAP50-95 is the Mean Average Precision (mAP) at IoU thresholds from 0.5 to 0.95. This is a more comprehensive metric than mAP50, as it takes into account the model’s performance at a wider range of IoU thresholds. Equations 4, 5, and 6 define the formulas for mAP50, mAP50-95, Recall, and Precision.\\n\\n\\\\[ \\\\text{mAP50} = \\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\text{AP}_{50}^{(i)} \\\\]\\n\\n(4)\\n\\n9\\nA PREPRINT - NOVEMBER 13, 2023\\n\\nTable 3: YOLO Performance Metrics\\n\\n| Model   | Class | Images | Instances | Box(P) | R     | mAP50 | mAP50-95 |\\n|---------|-------|--------|-----------|--------|-------|-------|----------|\\n| YOLOv8s | all   | 1644   | 1444      | 0.94   | 0.921 | 0.96  | 0.747    |\\n|         | 1     | 1644   | 1444      | 0.94   | 0.921 | 0.96  | 0.747    |\\n| YOLOv8m | all   | 1644   | 1444      | 0.938  | 0.927 | 0.959 | 0.766    |\\n|         | 1     | 1644   | 1444      | 0.938  | 0.927 | 0.959 | 0.766    |\\n| YOLOv5s | all   | 1644   | 1444      | 0.935  | 0.919 | 0.95  | 0.721    |\\n|         | 1     | 1644   | 1444      | 0.935  | 0.919 | 0.95  | 0.721    |\\n| YOLOv5m | all   | 1644   | 1444      | 0.932  | 0.931 | 0.958 | 0.748    |\\n|         | 1     | 1644   | 1444      | 0.932  | 0.931 | 0.958 | 0.748    |\\n\\n![YOLOv8s Loss Diagrams](image1.png)\\n![YOLOv8s Metrics Diagrams](image2.png)\\n\\nFigure 7: Comprehensive Loss and Metric Analysis for YOLOv8s in Cervical Vertebrae Spine Detection.\\n\\n\\\\[ \\\\text{mAP50-90} = \\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\text{AP}_{50-90}^{(i)} \\\\]\\n\\n\\\\[ \\\\text{Recall (R)} = \\\\frac{\\\\text{True Positives}}{\\\\text{True Positives + False Negatives}} \\\\]\\n\\n(5)\\n\\n(6)\\n\\n10![YOLOv8s Loss Diagrams](<image_path>)\\n\\n![YOLOv8s Metrics Diagrams](<image_path>)\\n\\nFigure 8: Comprehensive Loss and Metric Analysis for YOLOv8m in Cervical Vertebrae Spine Detection.\\n\\n![Confusion Matrix for YOLOv8s](<image_path>) ![Confusion Matrix for YOLOv8m](<image_path>)\\n\\nFigure 9: Confusion Matrices for YOLOv8s and YOLOv8m Models in the Detection of Cervical Vertebrae Fractures\\n\\n\\\\[ \\n\\\\text{Precision (P)} = \\\\frac{\\\\text{True Positives}}{\\\\text{True Positives} + \\\\text{False Positives}} \\\\quad (7)\\n\\\\]\\n\\nThe loss and metric diagrams for YOLOv8s and YOLOv8m are also presented in Figure 7 and Figure 8 respectively showing a downward trend. A curated set of predictions, exemplifying the proficiency of the YOLOv8s model in detecting cervical spine fractures, is visually depicted in Figure 10.\\n\\n11A PREPRINT - NOVEMBER 13, 2023\\n\\n![Figure 10: Visualization of YOLO Algorithm Predictions](figure10.png)\\n\\nFigure 10: Visualization of YOLO Algorithm Predictions on Cervical Vertebrae Spine Fracture Detection Dataset Images. Subfigures (a)-(f) display a selection of predictions, showcasing the YOLOv8 model's performance in detecting cervical spine fractures.\\n\\n4 Discussion\\n\\nThis paper presents a two-step pipeline aimed at detecting cervical vertebrae within individual image slices and locating fractures. In the initial stage, a multi-input network, which includes both the image data and associated image metadata, undergoes training. This network is constructed upon the Global Context Vision Transformer architecture and is evaluated in terms of performance, with a comparison to well-known deep learning image classification models. In the subsequent stage, a YOLOv8 model is trained specifically for fracture detection in the images, and its performance is assessed in relation to YOLOv5.\\n\\nOne of the notable strengths of the proposed network is its lower Coverage Error, which stands at 1.26. This indicates that it predicts fewer unnecessary labels compared to some of the other models, such as ViT with a Coverage Error of 1.41 and Convext with a Coverage Error of 1.35. This lower Coverage Error suggests that the proposed network produces more precise results, which is a significant advantage for this specific classification task.\\n\\nOn the downside, it’s important to consider the relatively high number of non-trainable parameters in the proposed network (14,683,998). High non-trainable parameter counts can lead to increased memory and computational requirements, potentially limiting its practicality in resource-constrained environments.\\n\\nOverall, the proposed network demonstrates strong performance in cervical spine vertebrae classification, with a competitive MacroF1 score, high Exact Match Ratio, and a notable advantage in terms of Coverage Error. However, the high number of non-trainable parameters is a potential drawback to be addressed for certain deployment scenarios. Careful consideration of the trade-offs between precision and model complexity is crucial when determining the suitability of the proposed network for specific applications.\\n\\n12\\nA PREPRINT - NOVEMBER 13, 2023\\n\\nFor fracture detection based on the results, it is clear that the performance of YOLOv8 outperforms YOLOv5. This is a significant improvement, especially considering that YOLOv8 is also faster than YOLOv5. Furthermore, although YOLOv8m has more parameters the mAP50 for YOLOv8s is a bit higher. On the other hand, YOLOv8m has higher mAP50-95. It is evident that YOLOv8s has demonstrated a strong ability to correctly classify images as “Normal” with 194 true positives and only 6 false negatives as presented in Figure 9.\\n\\nHowever, it tends to make more errors when classifying images as “Fracture,” as indicated by 97 false positives and 1347 true positives. This can be attributed to the nature of medical image analysis, where the cost of missing a “Fracture” (false negatives) may be considerably higher than misclassifying a “Normal” image as a “Fracture” (false positives). YOLOv8m, on the other hand, demonstrates a similar trend but with slightly improved performance when compared to YOLOv8s. It correctly classifies 192 “Normal” images and 1353 “Fracture” images. However, it still makes some errors, with 8 false negatives for “Normal” and 91 false positives for “Fracture”. This model appears to strike a better balance between precision and recall for both classes, indicating a more robust classification performance.\\n\\n5 Conclusion\\n\\nThe study introduced a two-stage pipeline leveraging deep learning models for the purpose of classifying vertebrae and detecting cervical spine fractures. The first stage incorporated a multi-input network with a Global Context Vision Transformer (GCViT) for vertebrae classification, while the second stage employed YOLOv8 for fracture detection. The outcomes are subsequently compared against existing deep learning-based image classification models, yielding noteworthy results. The proposed architecture demonstrated its efficacy, achieving a commendable 96% Macro FI accuracy for vertebrae classification and a Mean Average Precision (mAP) of 96% for fracture detection.\\n\\nIn terms of future research directions, it is worth exploring the potential integration of segmentation models to further enhance the precision of cervical spine fracture identification. Such segmentation models can facilitate the delineation of distinct anatomical structures within the cervical region, ultimately refining the diagnostic process.\\n\\nIn summary, this study focuses on investigating medical image diagnostics, specifically in the early and accurate identification of cervical spine fractures. The two-stage approach introduces significant advancements towards improving the management of critical medical injuries and alleviating the burden on radiologists.\\n\\n\",\n",
       " 'LONG-BONE FRACTURE DETECTION USING ARTIFICIAL NEURAL NETWORKS BASED ON LINE FEATURES OF X-RAY IMAGES\\nA PREPRINT\\n\\nAlice Yi Yang  \\nSchool of Electrical and Information Engineering  \\nUniversity of the Witwatersrand  \\nJohannesburg, South Africa, 2000  \\nyangalic8@gmail.com\\n\\nLing Cheng  \\nSchool of Electrical and Information Engineering  \\nUniversity of the Witwatersrand  \\nJohannesburg, South Africa, 2000  \\nling.cheng@wits.ac.za\\n\\nFebruary 21, 2019\\n\\nABSTRACT\\n\\nTwo line-based fracture detection scheme are developed and discussed, namely Standard line-based fracture detection and Adaptive Differential Parameter Optimized (ADPO) line-based fracture detection. The purpose for the two line-based fracture detection schemes is to detect fractured lines from X-ray images using extracted features based on recognised patterns to differentiate fractured lines from non-fractured lines. The difference between the two schemes is the detection of detailed lines. The ADPO scheme optimizes the parameters of the Probabilistic Hough Transform, such that granule lines within the fractured regions are detected, whereas the Standard scheme is unable to detect them. The lines are detected using the Probabilistic Hough Function, in which the detected lines are a representation of the image edge objects. The lines are given in the form of points, (x, y), which includes the starting and ending point. Based on the given line points, 13 features are extracted from each line, as a summary of line information. These features are used for fractured and non-fracture classification of the detected lines. The classification is carried out by the Artificial Neural Network (ANN). There are two evaluations that are employed to evaluate both the entirety of the system and the ANN. The Standard Scheme is capable of achieving an average accuracy of 74.25%, whilst the ADPO scheme achieved an average accuracy of 74.4%. The ADPO scheme is opted for over the Standard scheme, however it can be further improved with detected contours and its extracted features.\\n\\nKeywords\\n\\nArtificial Neural Network · Automated Diagnosis · X-ray Images · Line-based Feature Extraction · Image Processing\\n\\n1 Introduction\\n\\nThe application of Computer Aided Diagnosis (CAD) in the medical field has been introduced since the early 1970s [1, 2], in which the first CAD system utilised a decision tree analysis. Since the early 1970s, CAD systems have developed further and some even employ the use of Artificial Neural Networks (ANN). Siam, M Et al. [3] proposed an adaptive interface agent (AdAgen) for X-ray fracture detection. The interface AdAgen uses neural network to collaborate with trained agents. The neural network is used to build the software interface agent for the detection of fractures in long bones. A semi-intelligent system is provided by the software agent. The results obtained from the simulations indicates that the incorporated agents assists with the performance of the automated fracture detection in leg radiography. The general approach to classifying the presence of bone fracture involves mapping the data to one of several predefined classes. However, there are challenges presented in the classification techniques, which are due to information overload, size and dimension of the data [4]. A classification technique is defined as a systematic approach of processing input data by constructing classification models. Examples of classification techniques includes Decision\\nTree Classifiers, Rule-Based Classifiers, Neural Networks, Support Vector Machines and Naïve Bayes Classifiers. The authors of [5], proposes a four-step system that makes use of fusion-classification techniques to automate the detection of bone fracture specifically for leg bones (tibia). The four-steps includes preprocessing, segmentation, feature extraction and bone detection. The three classifiers during the fusion classification are Back-Propagation Neural Network (BPNN), Support Vector Machine (SVM) and Naïve Bayes Classifiers (NB). Through experimentation, the authors stated that the proposed four-step system showed significant improvement in terms of detection rate and speed of classification. An alternative classifier is the Convolutional Neural Network (CNN) which is a supervised classifier. Dojcenik A. Et al. [6] proposed a hybrid system with a CNN and Wavelet Transform-Singular Value Decomposition (DWT-SVD) for the classification of malignant and benign masses from liver CT images. The authors intension for the hybrid system is to reduce the execution time of the CNN architecture. The features for the classification are extracted using Perceptual hash functions. The hybrid system was evaluated using 200 images, in which 100 images are of benign tumours and the other 100 are of malignant tumours. The system achieved an accuracy of 97.3%. Features are crucial for the classification of various categories. The authors of [7] developed a model based on Deep Convolutional Neural Network (DCNN) for feature extractions. The features are extracted from X-ray images for the classification of weld flaw types. The model uses a technique called Sparse Autoencoder (SAE) as well as the mapping of the CNN layers for feature extraction. This extracted features are compared to the features from the traditional Grey Level Co-occurrence Matrix (GLCM) technique. The results obtained by the authors indicate that the extracted features based on the DCNN is better than traditional features, as it obtained a 97.2% accuracy whereas the accuracy obtained by the traditional methods obtained an 82.2% accuracy. The processing of images are essential as it ensures that all images are consistent to obtain the best results when passed through the system. In [8] the proposed system employs fuzzy network, multilevel threshold and morphological methods for the removal of noise and enhance the contrast of the CT images. The fuzzy network proved to balance the noise reduction and enhancement in the results, whilst the multilevel Otsu’s threshold filled in the missing gaps found within the images.\\n\\nThis paper describes a novel Adaptive Differential Parameter Optimization (ADPO) line-based fracture detection scheme. The purpose of the line-based fracture detection scheme is to offer a second opinion to medical physicians by classifying fractured and non-fractured lines present in the X-ray images. The proposed novel technique is intended to classify fractured lines from non-fractured lines, by training the Artificial Neural Network (ANN) with lines rather than the entire image. The proposed line-based fracture detection eliminates the need to train the ANN with entire images in order to understand the ADPO approach for line-based fracture detection, the Standard line-based fracture detection is detailed in Section 2.1. The Standard line-based fracture detection procedure extracts line features from the canny generated image using the Probabilistic Hough Transform. A total of 13 features are extracted from each detected line within the image. The features are detailed in Section 2, which includes distance, gradient, and the starting and ending points of the line. The features are labelled as either fractured or non-fractured using a graphical user interface (GUI). The labelled features are used to both train and test the ANN. The architecture of the ANN is described in Section 2.7, in which it consists of four layers, an output and input layer and two hidden layers. PCA is performed on the line features to determine the dominant contributing features that differentiates a fractured line from a non-fractured line. The results of the analysis indicates that the distance in the horizontal direction holds dominance for fractured lines, whilst the distance in the vertical direction holds dominance for non-fractured lines. Further PCA results are detailed in Section 2.6. The line classification of the ANN is detailed in Section 3, which includes two different experimental set-ups. The first is line-based with image context, in which it evaluates the overall system and the second is line-based without any image context. The second experimental set-up evaluates the performance of the ANN.\\n\\nSince the Standard line-based fracture detection approach does not detail all the granule lines found within the fractured area of the X-ray image, a novel ADPO scheme is proposed. This scheme optimizes the parameters of the Probabilistic Hough Transform, such that all detailed lines are detected for further data processing. There are three parameters that are optimized, namely the threshold, minimum line length, and maximum line gap parameters. The optimization and selected values for the three parameters are detailed in Section 4. The follow-up procedures of the ADPO is similar to the approach of the Standard line-based fracture detection, however the difference is the data that is given to the ANN. Only lines that fall within the leg-bone region of the leg are used to train the ANN, all other lines are disregarded as the lines are classified as knee, foot or flesh. The filtering technique of the bone from flesh lines in the leg area is described in Section 4.2. The performance of the ADPO line-based fracture detection system is detailed in Section 4.3, whereby it uses the same experimental set-up as the Standard scheme.\\n\\n2 Methodology\\n\\n2.1 Standard Line-Based Fracture Detection\\n\\nThe Standard line-based fracture detection follows the procedure in which it first extracts lines from the canny processed image. This is followed by the extraction of 13 features from each line. The features are used to train and test the\\nA PREPRINT - FEBRUARY 21, 2019\\n\\nANN. Additionally PCA is applied to the features to determine the dominant feature(s) that differentiate fractured and non-fractured lines. The training component of the ANN both sets-up the neural network as well as train it, whilst the execution component is employed for testing the ANN. The results obtained from the evaluation is analysed to assess the ANN’s performance. Figure 1 illustrates a graphical flow of the Standard line-based fracture detection procedure.\\n\\n![Flowchart](https://via.placeholder.com/600x400?text=Flowchart+Image+Missing)\\n\\nFigure 1: Flowchart illustrating the procedural flow of the Standard line-based fracture detection\\n\\n2.2 Image Enhancement\\n\\nAll raw X-ray images are converted to greyscale to simplify the image enhancement process, as greyscale images have a single channel compared to RGB where there are three channels. The image enhancement process consists of the removal of white space, pixel equalisation, gamma correction, denoising and unsharp masking. The purpose of the process is to create a high contrast between the long-bone edges and all other pixels within the image. The high contrast ensures that all the image edges are detected by the Canny edge detection operation. The Canny edge detection operation generates a binary image (black and white) with all the long-bone edges highlighted in the image [9]. The Canny edge detection operation is borrowed from the OpenCV2.4 library. The binary images are employed for the line extraction, which lines are extracted based on the edge image objects detected by the Canny edge detection operation.\\nA PREPRINT - FEBRUARY 21, 2019\\n\\n2.3 Line Extraction\\n\\nThe line extraction is performed by utilizing Probabilistic Hough Transform. The Probabilistic Hough Transform is a line detection technique in which lines are detected from contrasted image produced by the Canny edge detection technique [10, 11]. It uses the Polar system in which a line is expressed in the form shown in (1). A line is defined by rearranging (1) to generate (2) for point P(x, y). Therefore, each line that passes through \\\\((x_i, y_i)\\\\) is represented by \\\\((r_\\\\theta, \\\\theta)\\\\).\\n\\n\\\\[ y = \\\\left(- \\\\frac{\\\\cos \\\\theta}{\\\\sin \\\\theta} \\\\right)x + \\\\left( \\\\frac{r_\\\\theta}{\\\\sin \\\\theta} \\\\right) \\\\tag{1} \\\\]\\n\\n\\\\[ r_\\\\theta = x \\\\cos \\\\theta + y \\\\sin \\\\theta \\\\tag{2} \\\\]\\n\\nThe \\\\((r_\\\\theta, \\\\theta)\\\\) coordinates are used to detect lines by determining the number of intersections between the curves. An increase in the number of intersections indicates a long line. Therefore, a threshold for the minimum number of intersection is defined for line detection. This is the operation of the Hough Transform, in which it tracks the number of intersections for each \\\\((r_\\\\theta, \\\\theta)\\\\). Detected lines by the Probabilistic Hough Transform are represented in the form of \\\\((x_1, y_1; x_2, y_2)\\\\). The chosen parameter values for the Probabilistic Hough Transform for line detection in the Standard scheme are listed in Table 1. The result of the detected lines by the Probabilistic Hough Transform is shown in Figure 2(b).\\n\\n![Figure 2: X-ray images illustrating the detected lines of the Probabilistic Hough Transform with the assigned parameters given in Table 1](fig2.png)\\n\\n(a) Enhanced X-ray image  (b) Image of detected lines\\n\\nFigure 2: X-ray images illustrating the detected lines of the Probabilistic Hough Transform with the assigned parameters given in Table 1\\n\\n4\\nA PREPRINT - FEBRUARY 21, 2019\\n\\nTable 1: The detailed Probabilistic Hough Transform parameters for Standard line-based fracture detection approach, along with the reasoning for the selected values.\\n\\n| Parameter          | Functionality                                                  | Assigned Value | Reasoning                                                                                                                                               |\\n|--------------------|----------------------------------------------------------------|----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| rho, ρ             | pixel accumulator for distance resolution of line detection    | 1              | The value is assigned to 1 for fine pixel resolution for line detection.                                                                                |\\n| theta, θ           | angle accumulator for angle resolution in line detection       | \\\\(\\\\frac{\\\\pi}{180}\\\\)  | The angle selected allows for all line angles to be considered for line detection.                                                                       |\\n| threshold          | only accumulators with a value above the threshold are considered as lines | 10             | The threshold value at 10 produces lines with sufficient detailing such that the lines in the fractured region are detected, whilst still maintaining a minimal number of detected lines. |\\n| minimum line length| provides an acceptable minimum line length. Lines that do not meet this requirement are rejected | 25             | The chosen value assists with eliminating lines that are too short. The short lines are product of the remaining noise from the enhanced image.          |\\n| maximum line gap   | defines the maximum line gap between points that link the same line | 10             | The value assigned to the parameter is chosen based on the generated lines, whereby the lines generated cover all critical information within the image but does not over extend the generated lines. An increased value, generates extended lines which implies that there is existing information that is not present in the image, whereas a decreased value shortens the lines and misinterprets the image information. |\\n\\n2.4 Line-based Feature Extraction\\n\\nA set of 13 features are extracted from each detected line. The features are a summarised representation of the lines detected from image edge objects found in the X-ray image. These features are used as inputs into the ANN. The purpose of extracting the features is to quantify and provide crucial information about the lines to the ANN that differentiates a fractured line from a non-fractured line. The quantification of the features reduces the complexity of the ANN in both training and execution. The features are extracted based on the starting and ending points in the form of \\\\(L(x_1, y_1, x_2, y_2)\\\\), provided by the Probabilistic Hough Transform. These extracted features are listed and detailed in Table 2.\\n\\n5A PREPRINT - FEBRUARY 21, 2019\\n\\nTable 2: The details of the extracted line features along with the feature notation and extraction methodology\\n\\n| Extracted Feature | Notation | Abv. | Extraction Methodology |\\n|-------------------|----------|------|------------------------|\\n| 1 x start         | x₁       | XI   | x₁ is the x-value of the starting point |\\n| 2 y start         | y₁       | YI   | y₁ is the y-value of the starting point |\\n| 3 x end           | x₂       | X2   | x₂ is the x-value of the ending point, where x₂ > x₁ |\\n| 4 y end           | y₂       | Y2   | y₂ is the y-value of the ending point |\\n| 5 distance        | d        | DIST | The distance feature is extracted using x₁, y₁, x₂, and y₂. The distance calculation is expressed in (3). |\\n|                   |          |      | ![equation](d = \\\\sqrt{(x_{2} - x_{1})^2 + (y_{2} - y_{1})^2} \\\\tag{3}) |\\n| 6 gradient        | θₗ       | G    | The gradient feature is determined using x₁, y₁, x₂, and y₂. It is determined using (4). |\\n|                   |          |      | ![equation](θ_{l} = \\\\tan^{-1} \\\\left( \\\\frac{x_{2} - x_{1}}{y_{2} - y_{1}} \\\\right) \\\\tag{4}) |\\n| 7 x-Midpoint      | xₘ       | X-MID| The x-Midpoint feature is determined using x₁ and x₂. The midpoint in the x-direction is calculated using (5). |\\n|                   |          |      | ![equation](x_{M} = \\\\frac{x_{1} + x_{2}}{2} \\\\tag{5}) |\\n| 8 y-Midpoint      | yₘ       | Y-MID| The y-Midpoint feature is calculated using y₁ and y₂. The midpoint in the y-direction is calculated using (6). |\\n|                   |          |      | ![equation](y_{M} = \\\\frac{y_{1} + y_{2}}{2} \\\\tag{6}) |\\n| 9 x-Difference    | ∆x       | X-DIFF| The x-Difference feature is determined using x₁ and x₂. It determines the difference between the x-values. |\\n|                   |          |      | ![equation](\\\\Delta x = x_{2} - x_{1} \\\\tag{7}) |\\n| 10 y-Difference   | ∆y       | Y-DIFF| The y-Difference feature is determined using y₁ and y₂, in which it determines the difference between the y-values. |\\n|                   |          |      | ![equation](\\\\Delta y = y_{2} - y_{1} \\\\tag{8}) |\\n| 11 x-distance     | dₓ       | X-DIST| This feature is derived from the Pythagoras theorem using (9), but the distance in the x-direction. |\\n|                   |          |      | ![equation](d_{x} = d \\\\cos{(θ_{l})} \\\\tag{9}) |\\n| 12 y-distance     | dᵧ       | Y-DIST| The y-distance feature similar to the x-distance feature, however it is given in the y-direction. The y-direction is extracted using (10). |\\n|                   |          |      | ![equation](d_{y} = d \\\\sin{(θ_{l})} \\\\tag{10}) |\\n| 13 gradient deviation | ∆θ    | G-DEV| The gradient deviation feature indicates the amount that the current gradient deviates from the most frequently occurred gradient, θₖᵉœₗ. The gradient deviation is obtained using (11). |\\n|                   |          |      | ![equation](\\\\Delta \\\\theta = | θ_{ref} - θ_{l} | \\\\tag{11}) |\\n\\n2.5 Feature Correlation Analysis\\n\\nThe Pearson product-moment correlation coefficient is used to measure the dependency between each feature extracted from 39,224 lines. The correlation coefficient, c is calculated using (12). The correlation coefficient is an indication\\nA PREPRINT - FEBRUARY 21, 2019\\n\\nof the strength of the association between two features. It is given between a range of -1 to 1, which quantifies both strength and direction of the association between two features. The value “-1” indicates a strong negative relation. This means that as one feature increases in value, the other decreases. A “0” value shows that there is no association, and the \"1\" indicates a strong positive relation between two features, as either feature value increases, the other increases as well.\\n\\n\\\\[\\nc = \\\\frac{cov(x, y)}{\\\\sqrt{s^2_x \\\\cdot s^2_y}},\\n\\\\]\\n\\n(12)\\n\\nwhere \\\\(cov(x, y)\\\\) is the covariance of \\\\(x\\\\) and \\\\(y\\\\) and \\\\(s^2_x\\\\) and \\\\(s^2_y\\\\) are the sample variances of \\\\(x\\\\) and \\\\(y\\\\). The covariance and samples are defined as follows:\\n\\n\\\\[\\ncov(x, y) = \\\\frac{\\\\sum (x - \\\\bar{x})(y - \\\\bar{y})}{n-1}\\n\\\\]\\n\\n\\\\[\\ns^2_x = \\\\frac{\\\\sum (x - \\\\bar{x})^2}{n-1}\\n\\\\]\\n\\n\\\\[\\ns^2_y = \\\\frac{\\\\sum (x - \\\\bar{x})^2}{n-1}\\n\\\\]\\n\\nThe purpose of a correlation map is to provide a visual association between each extracted line feature. The association provides an indication of the features that are dependent on one another, as well as features that are independent from all other features. The correlation map for the 13 extracted line features is illustrated in Figure 3. The correlation map indicates that the gradient deviation and x-Difference feature are less reliant on all other features and are more independent as they their relations with the other features are either weak positive or weak negative. Independent features are crucial as they have minimal redundant information. This allows for some discrepancy in the input information used to train the ANN. Redundant features contain duplicated information and as a result, they do not provide enough information to allow for the differentiation between fracture lines and non-fractured lines. The correlation map presented in Figure 3 contains a minimal amount of features that have a strong positive (light blocks) or strong negative (dark blocks) association with other features.\\n\\n\\\\[\\n\\\\begin{array}{cccccccccccccc}\\n\\\\text{X1} & \\\\text{1} & \\\\text{0.09} & \\\\text{0.99} & \\\\text{0.11} & \\\\text{-0.09} & \\\\text{-0.11} & \\\\text{1} & \\\\text{-0.1} & \\\\text{0.07} & \\\\text{0.06} & \\\\text{0.16} & \\\\text{0.15} & \\\\text{0.019} \\\\\\\\\\n\\\\text{Y1} & 0.09 & \\\\text{1} & \\\\text{-0.04} & \\\\text{0.98} & \\\\text{0.32} & \\\\text{-0.09} & \\\\text{-0.056} & \\\\text{0.99} & \\\\text{-0.12} & \\\\text{-0.21} & \\\\text{0.16} & \\\\text{-0.23} & \\\\text{0.27} \\\\\\\\\\n\\\\text{X2} & 0.99 & -0.04 & \\\\text{1} & \\\\text{0.063} & \\\\text{0.056} & \\\\text{0.11} & \\\\text{1} & \\\\text{0.052} & \\\\text{0.03} & \\\\text{-0.062} & \\\\text{0.11} & \\\\text{0.074} & \\\\text{0.034} \\\\\\\\\\n\\\\text{Y2} & 0.11 & 0.98 & 0.063 & \\\\text{1} & \\\\text{0.33} & \\\\text{0.12} & \\\\text{-0.099} & \\\\text{0.9} & \\\\text{-0.1} & \\\\text{-0.4} & \\\\text{-0.03} & \\\\text{-0.5} & \\\\text{-0.84} \\\\\\\\\\n\\\\text{X-DIFF} & -0.09 & 0.32 & 0.056 & 0.33 & \\\\text{1} & 0.017 & -0.022 & \\\\text{0.33} & \\\\text{0.25} & \\\\text{0.049} & \\\\text{0.32} & \\\\text{0.51} & \\\\text{0.34} \\\\\\\\\\n\\\\text{Y-DIFF} & -0.11 & -0.09 & 0.11 & 0.12 & 0.017 & 1 & 1 & 0.5 & 0.68 & 0.58 & 0.68 & 0.48 & 0.47 \\\\\\\\\\n\\\\text{X-MID} & -1 & -0.08 & 1 & 0.05 & 0.022 & 0.11 & 1 & 0.77 & 0.053 & 0.13 & 0.11 & 0.07 \\\\\\\\\\n\\\\text{Y-MID} & -0.1 & 0.99 & 0.052 & 0.99 & 0.033 & 0.015 & 0.077 & 1 & -0.11 & -0.15 & 0.23 & -0.28 & 0.24 \\\\\\\\\\n\\\\text{DIST} & 0.007 & -0.12 & -0.062 & -0.042 & -0.049 & 0.58 & 0.058 & -0.115 & 1 & 0.68 & 0.29 & 0.67 & -0.07 \\\\\\\\\\n\\\\text{Y-DIST} & 0.055 & -0.21 & 0.062 & -0.048 & 0.049 & 0.58 & 0.058 & -0.15 & 0.68 & 1 & 0.18 & 0.43 & 0.43 \\\\\\\\\\n\\\\text{X-DIST} & -0.16 & 0.16 & -0.11 & 0.3 & 0.32 & 0.16 & -0.13 & 0.23 & 0.29 & 0.18 & 1 & -0.83 & -0.018 \\\\\\\\\\n\\\\text{G} & 0.15 & -0.23 & 0.074 & -0.35 & -0.48 & 0.11 & -0.28 & 0.087 & -0.35 & 0.43 & -0.83 & 1 & 0.31 \\\\\\\\\\n\\\\text{G-DEV} & 0.019 & 0.27 & 0.034 & 0.21 & 0.034 & -0.27 & 0.007 & 0.24 & -0.07 & 0.43 & -0.018 & 0.55 & 1 \\\\\\\\\\n\\\\end{array}\\n\\\\]\\n\\nFigure 3: Correlation Map illustrating the association between each of the 13 features extracted\\n\\n7\\nA PREPRINT - FEBRUARY 21, 2019\\n\\n2.6 Principal Component Analysis\\n\\nPrincipal Component Analysis (PCA) is a linear technique that performs dimensionality reduction through the process of embedding the data into a linear subspace of low-dimensionality. The low-dimensional representation describes the variance found within the data [12]. The purpose of utilizing PCA in this paper is to identify the dominant line features to determine the main contributing factors that distinguishes whether a line is considered fractured or non-fractured. PCA operates in such a manner that the feature with the most variation is considered a dominant feature. The variation of the features is one of the contributing factors for the classification of fractured and non-fractured lines. The variation of a feature indicates that the feature values varies for different line characteristics. PCA is applied to a number of lines, _m_, where _m_ = 39,224 for a number of features, _n_ where _n_ = 13. A matrix, _X_ with the dimensions of _m_ × _n_ is created to represent each feature of each line. The matrix is normalised to generate matrix, _X\\'_ in which PCA is applied to. PCA produces a _n_ × _n_ covariance matrix, _A_. The eigenvalues, _λ_ and eigenvectors, _v_ are generated using the covariance matrix. The eigenvalue, _λ<sub>i</sub>_ is a scaler which provides the magnitude of the varying values for each _i_-th feature, where _i_ ∈ {1, 2, 3, ..., _n_}. The higher the scaler value, the more variation the feature contains, whereas a lower scaler value shows that there is little to no variation within the feature. However, it is difficult to assign the appropriate eigenvalue to its associated feature as the eigenvalues are sorted from largest to smallest due to the PCA procedure. Therefore, the eigenvectors are utilized, as each element in the eigenvector is ordered in the same manner as the columns of the matrix, _X\\'_. Thus, the _i_-th feature is represented by element, _ɛ<sub>ij</sub>_ in eigenvector, _e<sub>j</sub>_. The determination of the contribution for each feature is entailed as follows:\\n\\n1. Sum all elements in eigenvector, _e<sub>j</sub>_ to generate a vector of summed elements, _s_ = {_s<sub>1</sub>, _s<sub>2</sub>, _s<sub>3</sub>, ..., _s<sub>n</sub>_}, where _n_ is the number of eigenvectors. The calculation of _s<sub>j</sub>_ for the _j_-th eigenvector is expressed in (13).\\n\\n   \\\\[\\n   s_j = \\\\sum_{i=1}^{n} \\\\epsilon_{ij}\\n   \\\\]\\n\\n2. Utilizing _s_, convert each element in the eigenvectors to a ratio, _r<sub>ij</sub>_. The eigenvectors are arranged in such a way the elements match matrix, _R_ with elements _r<sub>ij</sub>_. Each element, _r<sub>ij</sub>_ is calculated using (14).\\n\\n   \\\\[\\n   r_{ij} = \\\\frac{\\\\epsilon_{ij}}{s_j}\\n   \\\\]\\n\\n3. The overall feature contribution, _c<sub>i</sub>_ obtained by averaging all elements in the _i_-th row of matrix, _R_. This calculation is expressed in (15). The feature contribution is associated to the _n_ extracted features from the lines.\\n\\n   \\\\[\\n   c_j = \\\\frac{\\\\sum_{j=1}^{n} r_{ij}}{n}\\n   \\\\]\\n\\nFigures 4, 5, and 6 illustrates the results of the PCA feature contribution. These results are crucial as it shows the indicative feature(s) that contribute most to the differentiation of a fractured and non-fractured line. Additionally, it is an indication of the feature that holds the most information about the line characteristics. Figure 4 shows the feature contribution for 39,224 extracted lines from 53 images. Figure 4 disregards the labeling of the lines. The most dominant feature in Figure 4 is gradient deviation with 76.36% contribution, followed by the gradient and x-distance feature. Therefore, despite the labelling of the lines, the gradient deviation, gradient and x-distance has the most varying values from the extracted lines. Thus, the three features holds crucial information about the line characteristics.\\n\\nFigure 5 presents the feature contribution for 15,561 extracted line features that are labelled as fractures. The results show that there are three very distinct features that are dominant for fractured lines, namely, x-distance, gradient deviation and x-difference. The feature x-distance has the most varying values for fractured lines. This is valid as the orientation of fractured lines are generally positioned in a horizontal direction. Additionally, the feature x-difference is valid, because the fractured lines vary greatly in vertical length. For the x-difference feature, this indicates that the fractured lines vary greatly in length in the horizontal direction.\\n\\nFigure 6 shows the three main dominant contributing features for non-fractured lines, which are x-difference, gradient deviation, and x-distance. The results indicate that the fractured and non-fractured features share two dominant features, namely gradient deviation and x-difference. This can cause confusion to the ANN in the classification of fractured and non-fractured lines, however the features differ in contribution for fractured and non-fractured lines. The results further shows that there are minor dominant features for non-fractured lines, namely, x-difference, y-distance, and gradient to assist with the differentiation between fractured and non-fractured lines.\\n\\nThe application of the PCA provides sufficient detailing about the contributions of each feature for both the fractured and non-fractured lines, as well as all lines in general. There are features that have minimal contribution to the line\\ninformation in which it can be eliminated to reduce the training and execution complexity of the ANN. However, with the potential of confusion between the fractured and non-fractured lines all extracted features are considered given that there are only 13 features providing crucial information about the extracted line.\\n\\n![Feature contribution percentage from PCA for all lines regardless of labelling](#)\\n\\nFigure 4: Histogram illustrating the results of the feature contribution from PCA for all extracted lines regardless of line classification\\n\\n![Feature contribution percentage from PCA for lines labelled as fractures](#)\\n\\nFigure 5: Histogram illustrating the results of the feature contribution from PCA for extracted lines labelled as fractured\\n\\n9\\n![Feature contribution percentage from PCA for lines labelled as non-fractures](Figure%206.png)\\n\\nFigure 6: Histogram illustrating the results of the feature contribution from PCA for extracted lines labelled as non-fractured\\n\\n### 2.7 Neural Network Structure\\n\\nThe architecture of the ANN for the Standard line-based fracture detection scheme consists of four layers: one input and output layer, and two hidden layers. The number of nodes, \\\\( n \\\\), in the input layer is 16. There are three additional nodes compared to the number of extracted line features in the ANN input layer. The three additional nodes are for the labeling of the X-ray image regions, namely, the knee, leg and foot region. Thus, if the line belongs within the leg region, the value “1” is assigned to the associated leg feature indicator, whilst a “0” value is assigned to both knee and foot feature indicator. The assignment of the knee, leg and foot region is performed during line feature extraction. Therefore, the knee, leg and foot region assignments are considered as additional features to the extracted line features.\\n\\nThere are two hidden layers in the architecture of the ANN. The number of layers within the neural network defines the ANN complexity [13]. An increased number of hidden layers increases training complexity of the ANN, since it will increase the number of iterations until the desired error is obtained [14]. Furthermore, increasing the number of hidden layers does not yield a higher accuracy from the ANN. However for a single hidden layer, the ANN is over simplified and is prone to over-fitting. Therefore, increasing the difficulty of obtaining a high accuracy. This leads to the selection of having two hidden layers for the architecture of the ANN for Standard line-based fracture detection scheme. Each hidden layer has \\\\( n - 1 \\\\) nodes. The purpose of the additional node in the hidden layer compared to the input layer is to introduce an additional vote of input before reaching the output layer [13][15].\\n\\nThe output layer only has one node, the outcome of the output node is expressed as \\\\( O_f \\\\), where \\\\(-1 \\\\leq O_f \\\\leq 1 \\\\). The range between “-1” to “1” is defined by the labeling of the output data, whereby fractured lines are assigned to “1” as its target output and non-fractured lines are assigned to “-1”. The details of the data labelling process is discussed in Section 2.8. The final detection outcome, O classifies the input line as fracture or non-fracture with “true” or “false. Thus, \\\\( O \\\\in \\\\{ \\\\text{true}, \\\\text{false} \\\\} \\\\) and is expressed in (16), where true defines a fracture and false defines a non-fracture. The ANN training and network set-up is detailed in Table 3.\\n\\n\\\\[ \\nO = \\n\\\\begin{cases} \\n\\\\text{true}, & 0 \\\\leq O_f \\\\leq 1 \\\\\\\\\\n\\\\text{false}, & -1 \\\\leq O_f < 0 \\n\\\\end{cases} \\n\\\\]\\n\\n(16)\\n**A PREPRINT - FEBRUARY 21, 2019**\\n\\nTable 3: The ANN training set-up for the Standard line-based fracture detection approach\\n\\n| ANN Detail Set-Up | Functionality | Assigned Value |\\n|-------------------|--------------|----------------|\\n| No. of ANN epochs | The number of epochs defines the maximum number of iterations allowed during the training of the ANN. If the error of the ANN does not meet the desired error value, the network continues training until the number of iterations has reached the defined epochs value | 50,000 epochs. |\\n| Desired Error | The desired error value is the minimal error allowed for the ANN during the training, as weights cannot perfectly match the input to output value, whilst still maintaining generality over large data sets. | 0.0001 |\\n| Training Data Order | The data given to the ANN for training is potentially ordered. An ordered training data set can swing the ANN weights from one decision to another. Thus, the training data is shuffled such that there is no favouritism given over a selected set of data. | “shuffled” |\\n\\n![Diagram](./image.png)\\n\\nFigure 7: Diagram showing the artificial neural network architecture for Standard line-based fracture detection scheme, where μ is indicative of the hidden layers.\\nA PREPRINT - FEBRUARY 21, 2019\\n\\n2.8 Data Line Labelling\\n\\nThe data labelling is performed using a graphical user interface (GUI). The data is labelled visually through the GUI by a user. The extracted lines from the Probabilistic Hough Transform is drawn onto the enhanced image of the original X-ray image, shown in Figure 8. Before the lines are labelled, further processing is performed to isolate the lines within the knee and foot region from the lines in the leg region. Thus, the labelling process is restricted to the lines in the leg region.\\n\\n![Figure 8](image-url)\\n\\nFigure 8: Images illustrating the labelling of the detected lines\\n\\nThe labelling process is performed manually by a human professional, whereby the user selects the region of the fracture. The lines in the selected region are labelled as fractures while the remaining lines are labelled as non-fractures. The regional labelling approach is chosen, as it mimics the visual detection approach that a medical professional utilises. This approach is chosen over the labelling of individual lines, as it is far too time-consuming and impractical. The impracticality of labelling individual lines stems from the visual information that a single line provides. A single line provides little to no evidence of whether it is fractured or non-fractured. Whereas, a group of lines provides more context about the line classification. The regional data labelling approach provides adequate information about the lines based on its location and neighbouring lines. A line is considered a fracture if either its starting or ending point is within the selected area. However, this approach is not perfect, as it mislabels lines that are non-fractures but fall within the selected region as fractures.\\n\\n3 Results of the Standard Line-Based Fracture Detection\\n\\n3.1 Artificial Neural Network Experimental Set-Up\\n\\nThe ANN is evaluated based on its performance to make accurate fracture and non-fracture classification. The set-up of the evaluation consists of both system and ANN evaluation. For the system evaluation, a total of 20 images are used to evaluate the system. There are 20 individual image cases for the system evaluation. Each case trains the ANN with a number of images ranging from 1 to 20. This evaluation provides the ANN with lines that has context about the image it is extracted from. The images, *i*, are randomly selected from a set of training images, *n*. Hence, *i* ∈ {1, 2, 3, ..., *n*}, where *n* = 29. There are an average of 740 lines per image, and the average ratio of fractures to non-fractures is 1 : 1.52. Each case has 10 simulations. This is to obtain an average accuracy for the system as each simulation accuracy\\n\\n12A PREPRINT - FEBRUARY 21, 2019\\n\\noutcome differs slightly from one another. The slight differences are a result of the randomly initialised weights in the ANN as well as the randomly selected images used for each training case. 23 images are used to test the system for each case to ensure that each case and simulation is evaluated fairly.\\n\\n3.2 System and Artificial Neural Network Results\\n\\nThe results of the system evaluation is presented in Table 4, whereby the minimum, average and maximum accuracies for each case is detailed. The system was evaluated using a total of 11,910 lines that are extracted from 23 images. From the 11,910 lines, 4,707 lines are fractured and 7,203 are non-fractured lines.\\n\\nTable 4: The results for the system’s minimum, average and maximum accuracy of the system for 20 cases over 10 simulations for the Standard line-based fracture detection scheme\\n\\n| No. Trained Images, \\\\(c_i\\\\) | Min Accuracy (%) | Average Accuracy (%) | Max Accuracy (%) |\\n|-----------------------------|------------------|----------------------|------------------|\\n| 1                           | 68.606           | 72.764               | 75.777           |\\n| 2                           | 60.462           | 69.9842              | 75.206           |\\n| 3                           | 60.831           | 71.0134              | 75.113           |\\n| 4                           | 66.255           | 71.8111              | 75.718           |\\n| 5                           | 60.336           | 70.1906              | 75.407           |\\n| 6                           | 66.314           | 72.1956              | 75.466           |\\n| 7                           | 69.639           | 72.2939              | 75.743           |\\n| 8                           | 60.571           | 71.3233              | 75.919           |\\n| 9                           | 67.674           | 71.7062              | 75.743           |\\n| 10                          | 68.585           | 72.7254              | 75.449           |\\n| 11                          | 67.674           | 72.5499              | 75.76            |\\n| 12                          | 67.599           | 71.5242              | 75.332           |\\n| 13                          | 69.018           | 72.4467              | 74.845           |\\n| 14                          | 72.704           | 74.2135              | 75.651           |\\n| 15                          | 60.579           | 69.7364              | 75.136           |\\n| 16                          | 60.487           | 70.0982              | 74.433           |\\n| 17                          | 67.389           | 72.6766              | 75.306           |\\n| 18                          | 61.226           | 69.519               | 75.928           |\\n| 19                          | 60.437           | 68.9798              | 75.55            |\\n| 20                          | 70.848           | 73.7094              | 75.197           |\\n\\nThe ROC curve is constructed using the sensitivity and specificity results from the binary classification system. There are four variables that are considered for the sensitivity and specificity, namely, true positive, false negative, false positive and true negative. Both false negative and false positive are miss-classifications of the lines and have detrimental consequences within the medical field. However, a false negative is far more severe than a false positive, as it implies that a fracture has been missed and can potentially be untreated leading to dire consequences. The calculation for both sensitivity and specificity are expressed in [17] and [18] respectively. The receiver operating characteristic (ROC) curve is a tool commonly utilised for performance evaluation for binary classification systems [16]. The ROC curve for the image evaluation is illustrated in Figure 10. The ROC curve is determined by employing curve-fitting on the measured results. The area under the curve (AUC) for the presented ROC graph is 0.8149. An ideal AUC has a value of 1. Therefore, the system has a favourable true positive sensitivity detection.\\n\\n\\\\[\\n\\\\text{sensitivity} = \\\\frac{\\\\text{TP}}{\\\\text{TP} + \\\\text{FN}}\\n\\\\]\\n\\n\\\\[\\n\\\\text{specificity} = \\\\frac{\\\\text{TP}}{\\\\text{FP} + \\\\text{TN}}\\n\\\\]\\n\\nFor the ANN evaluation, each case is made up of a number of lines that is used to train the ANN to evaluate its performance. The lines are grouped into two categories, fractures and non-fractures. The evaluation is set-up such that there is an equal number of fractured and non-fractured lines for each case. Thus 50% of lines are fractured and 50% of lines are non-fractured. Each case consists of a number of both fractured and non-fractured lines in groups of 5’s. The total number of lines used for training is 1,500 lines. Consequently there are a total of 150 cases. The performance of the ANN is evaluated in the same manner as the system evaluation, whereby a fixed number of lines is used for testing the accuracy of the ANN. A total of 13,178 lines are used to test each case, whereby 5,162 are labelled as fractured lines and 8,016 are non-fractured lines. The result of the accuracy for each case is illustrated in Figure 11.\\n\\n13\\nA PREPRINT - FEBRUARY 21, 2019\\n\\nGraph showing the average accuracy results for the Standard line-based fracture detection scheme\\n\\n![Graph showing accuracy results](attachment:image.png)\\n\\nFigure 9: Graph illustrating the average accuracy for 20 cases over 10 simulations for the Standard line-based fracture detection scheme\\n\\nTable 4 shows that despite the number of images trained, it does not affect the accuracy of the ANN as the accuracy of the system ranges between 65.37% to 75.44%. This indicates that a single image provides the ANN with enough line training data to achieve an average of 71.5% accuracy. Figure 11 illustrates that accuracy of the ANN is between 67% to 72% after the exposure of 40 lines. Therefore, the minimum number of lines needed to achieve an accuracy between 67% to 72% are 20 fractured and 20 non-fractured lines. An increase in the number of lines exposed to the ANN, does not result in any drastic improvements regarding the detection accuracy.\\n\\n14\\nA PREPRINT - FEBRUARY 21, 2019\\n\\n![ROC curve illustrating the TPR and FPR for the Standard line-based fracture detection for image evaluation](#)\\n\\nFigure 10: Figure illustrating ROC curve for the Standard line-based fracture detection\\n\\nGraph illustrating the performance of the ANN for the Standard line-based fracture detection scheme\\n\\n![Graph illustrating the performance of the ANN whilst training it with an equal number of fractured and non-fractured lines for each case for the Standard line-based fracture detection scheme](#)\\n\\nFigure 11: Graph illustrating the performance of the ANN whilst training it with an equal number of fractured and non-fractured lines for each case for the Standard line-based fracture detection scheme\\n\\n15\\nA PREPRINT - FEBRUARY 21, 2019\\n\\n4 Adaptive Differential Parameter Optimized Line-Based Fracture Detection\\n\\nFor the Standard line-based fracture detection scheme, the parameters of the Probabilistic Hough Transform is not optimized for line detection. The purpose of the Adaptive Differential Parameter Optimized (ADPO) line-based fracture detection scheme is to optimize the parameters, such that the generated lines can accurately represent the image edge objects found in the X-ray image. This includes generating granule lines that details the fractures in the fractured region. There are three major parameters that are optimized: threshold, minimum line length and maximum line gap parameters. The ADPO line-based fracture detection scheme follows the same procedure as the Standard scheme. The difference is that the ADPO introduces a parameter optimizing process and a filtering technique to separate surrounding flesh lines from the leg-bone lines in the leg region. The architecture and training of the ANN remains the same as the Standard scheme. Additionally, the evaluation for both system and ANN remains the same.\\n\\n4.1 Adaptive Differential Parameter Optimization\\n\\n1. The threshold parameter controls the number of point intersections, such that \\\\((r_0, \\\\theta)\\\\) coordinates are considered as lines. An increased threshold value defines fewer lines with less minor details about the image edge objects, whereas a decreased threshold parameter value captures all details about the image. This is illustrated in Figure 12. Therefore, the threshold parameter value is set to 1. Although the detected lines in Figure 12(a) are scattered compared to the lines in Figure 12(b) the lines are reconstructed at a later stage when all other parameters are optimized.\\n\\n![Threshold value 1](image-a)\\n![Threshold value 50](image-b)\\n\\nFigure 12: Images illustrating the generated line difference between a minimal and increased threshold parameter value\\n\\n2. The minimum line length, \\\\(l_{min}\\\\) parameter controls the accepted length of the detected line. An increased \\\\(l_{min}\\\\) eliminates lines that are considered as noise, however it disregards detailed lines that are found in the fractured region. This is depicted in Figure 13 where Figure 13(a) shows more detailed lines, particularly within the fractured region compared to Figure 13(b) whereas in Figure 13(b) the outline of the fracture is barely visible from the generated lines. Although a minimal value assigned to \\\\(l_{min}\\\\) may be ideal, but the generated lines contains redundant information, as a number of short segmented lines can be represented by a single line instead. Additionally, the shorter lines distorts the most frequently occurring gradient within the leg region and therefore effects the gradient deviation feature.\\n\\n16\\n![Images](https://via.placeholder.com/150)\\n\\nFigure 13: Images illustrating the generated line difference between a minimal and increased value assigned to the minimum line length parameter\\n\\nThe optimized minimum line length, \\\\( l\\'_{\\\\min} \\\\), is determined by obtaining the gradients of each line, \\\\( L \\\\), detected in the image for a particular \\\\( l_{\\\\min} \\\\), where \\\\( 1 \\\\le l_{\\\\min} \\\\le 25 \\\\). Thus there are a total of 25 images generated for each minimum line length ranging from 1 to 25. The gradients of each line within each image is used to calculate the average gradient, \\\\( \\\\bar{\\\\theta}_{\\\\min} \\\\), to represent the general direction of the lines in the image. The determination of \\\\( \\\\bar{\\\\theta}_{\\\\min} \\\\) is expressed in (19). The average gradients are evaluated to determine the maximum difference between adjacent average gradients. This evaluation is performed by finding the difference between each adjacent average gradient, which is expressed in (20). The \\\\( l\\'_{\\\\min} \\\\) associated to the maximum difference between the average gradients, \\\\( \\\\Delta\\\\bar{\\\\theta}_{\\\\min} \\\\) is the optimized minimum line length. This is because the gradients of the fractured lines are in a more horizontal position between 0° and 45°, compared to non-fractured lines which are generally in placed in a vertical position. Therefore, the fractured lines have a lower gradient value than non-fractured lines and the largest average difference is associated with \\\\( l\\'_{\\\\min} \\\\). The calculation is expressed in (21).\\n\\n\\\\[\\n\\\\bar{\\\\theta}_{\\\\min} = \\\\frac{\\\\sum_{k=1}^{m} \\\\theta_{L_k}}{m}\\n\\\\]\\n(19)\\n\\nwhere, m is the total number of detected lines in the image and \\\\( L_k \\\\) is the k-th line.\\n\\n\\\\[\\n\\\\Delta\\\\bar{\\\\theta}_{\\\\min} = \\\\bar{\\\\theta}_{\\\\min} - \\\\bar{\\\\theta}_{\\\\min-1},\\n\\\\]\\n(20)\\n\\n\\\\[\\nl\\'_{\\\\min} = \\\\arg\\\\max(\\\\Delta\\\\bar{\\\\theta}_{\\\\min})\\n\\\\]\\n(21)\\n\\nHowever the lines, \\\\( L_{l\\'_{\\\\min}} \\\\), generated from the optimized minimum line length does not contain all the detailed lines in the fractured region. Thus, lines generated by prior minimum line lengths from the optimized minimum line length are borrowed to fill in the missing detailed fractured lines. A conditional statement is employed such that the borrowed minimum line length will never be less than the value “1”. The prior minimum line lengths are specifically, \\\\( l\\'_{\\\\min} - 1\\\\) and \\\\( l\\'_{\\\\min} - 2\\\\). Any further prior lines that are considered are redundant as it shares the same lines with \\\\( l\\'_{\\\\min} + 1 \\\\) and \\\\( l\\'_{\\\\min} - 2 \\\\). The considered lines, \\\\( L \\\\) are as described in (22).\\n\\n\\\\[\\nL = \\\\left[L_{l\\'_{\\\\min}}, \\\\, L_{l\\'_{\\\\min-1}}, \\\\, L_{l\\'_{\\\\min-2}}\\\\right]\\n\\\\]\\n(22)\\nwhere \\\\( L \\\\) is a vector of lines, \\\\( L(x_1, y_1, x_2, y_2) \\\\).\\n\\nThere are repeated lines from the three selected sets of line vectors: \\\\( L_{l_p,...} \\\\), \\\\( L_{r_p,...} \\\\), and \\\\( L_{y_p,...} \\\\). Only the unique lines are exacted and considered. Thus, the final lines considered, \\\\( L_f \\\\) are expressed in (23)\\n\\n\\\\[\\nL_f(L) = \\\\text{unique}\\\\{L_i\\\\}_{i \\\\in \\\\{1,2,3,...,n\\\\}}\\n\\\\]\\n\\n(23)\\n\\n3. The maximum line gap, \\\\( L_{g_{max}} \\\\) parameter controls the gap between the ending points of each line segment. If the distance between the line segments’ ending points do not meet the allowed maximum line gap, the segments are combined to create a single line. A decreased maximum line gap generates lines that does not describe the image edge objects in the fractured region, despite the optimized minimum line length value. The results are shown in Figure 14(a). An increased maximum line gap generates lines that misrepresents the image edge object, as it over extends the lines. The result of an increased maximum line gap value is shown in Figure 14(b) The selected value for the maximum line gap is 13. The value is chosen based on the evaluation of the number of lines generated for each maximum line gap value ranging between 10 and 20. The maximum line gap value of 13 is chosen as it generates a sufficient number of lines that details the crucial information for 50 images used for testing.\\n\\n![Figure: Comparison of line gaps](image)\\n\\n**Figure 14: Images illustrating the generate line difference between a minimal and increased value assigned to the maximum line gap parameter**\\n\\n4.2 Lines of Interest in the Leg Region\\n\\nThrough visual analysis of lines generated from the X-ray images, the detected lines includes both the lines of the bones and of the surrounding flesh (outer leg) as illustrated in Figure 15(a). The fractured lines are only situated in the leg-bone region. Therefore, the focus of the fracture detection only concerns the lines within the leg-bone region. Consequently, the surrounding flesh lines are eliminated to reduce the number of lines used for training the ANN. The elimination of the flesh lines is performed through the process of analysing the x-values of each line point. The x-values are chosen for the analysis for two reasons. The first is to isolate the lines in the leg-bone region from the surrounding flesh lines by creating vertical slices of the image. The second reason is due to the pattern discovered with the x-values regarding its difference in density for lines in the flesh and leg-bone region. The leg-bone has a higher x-value density compared to the flesh lines, as there are more detected lines in the leg-bone region. This pattern holds true for all X-ray images used for this research.\\n\\n18\\n![Image](attachment:image.png)\\n\\nFigure 15: Images illustrating the before and after of applying the sliding window to isolate the surrounding flesh lines from the leg-bone lines in the leg region\\n\\nIn order to focus on the lines in leg-bone region, the lower and upper bounds are determined using the extracted x-values from the lines. Therefore, x1 and x2 are extracted from each line, L(x1, y1, x2, y2). A frequency vector, f, for all unique x-values is created. The frequency vector holds the total number of occurrences for each unique x-value in the form of (x, f), where x is the unique x-value and f is the number of occurrences for the particular x-value. The x-values of f, f_x, are analysed by employing a sliding window, w, that traverses through each x-value of the image width, w, to find matching values in f_x. The window size, l_w, of the sliding window is 5% of the image width, w. The 5% value is selected based on the number of detailed points obtained from the windowing procedure, which is through trial-and-error. An increase in the window size gives leeway for the lower bound value, however the upper bound value is shortened and does not achieve the desired x-value. Thus, by increasing the window size, the ROI is shifted.\\n\\nThe sliding window is bounded by two variables, the starting window value, w_s, and the ending window value, w_e. The two variables vary in value as the sliding window traverses through the width of the image, whilst maintaining a fixed length defined by l_w. If a match between the i-th x-value within the sliding window (w_s ≤ i ≤ w_e) with the j-th x-value in f_x, then frequency, f_j is summed to obtain the window total frequency, f(w) for the i-th width x-value. The purpose for obtaining f(i) for each i-th x-value is to find the region with the highest x-value density to determine the lower and upper bounds of the leg-bone region. The described algorithm is presented in Algorithm 1 and the result of the density algorithm for Figure 15(a) is illustrated in Figure 16.\\n\\nIn Figure 16 the region with the highest peak is indicative of the highest x-value density within the image. The lower and upper bounds of the region is determined by observing the turning points of the graph in Figure 16. The result of applying the lower and upper bounds is presented in Figure 15(b).\\n\\n4.3  Optimized Line-Based Detection Results\\n\\nThe results of the APPO line-based fracture detection scheme are obtained from the same experimental set-up described in Section 3.1. However, the difference is the data used for both training and testing the ANN. The ANN is only trained with lines that are found within the leg-bone region. The elimination of the surrounding flesh lines reduces the training complexity and of the ANN. Therefore, the evaluation of the ANN is focused on classifying fractured lines from non-fractured lines found only in the leg-bone region, whilst all other lines are evaluated by other components.\\nA PREPRINT - FEBRUARY 21, 2019\\n\\n---\\n\\nAlgorithm 1: Obtaining x-Value-Density of x-values Extracted from Lines Algorithm  \\nData: x-values, x₁ and x₂ from each detected line  \\nResult: csv file with summed frequency values at each i-th value within the X-ray image  \\nCreate csv file;  \\nObtain frequency vector fᵢ for each unique x-value through frequency analysis;  \\nInitialise sliding window size, lₕᵥ, to 5% of image width, wᵢ;  \\nfor i = 0, j < wᵢ, i + d₀  \\n  Window start, wₛ = i;  \\n  Window end, wₑ = i + lₕᵥ;  \\n  Window frequency, fᵢ⁽⁰⁾ = 0;  \\n  for j = 0, j < l fⱼ, j + + do  \\n    if wₛ ≤ l fⱼ⁽⁰⁾ ≤ wₑ then  \\n      fᵢ⁽⁰⁾ = fᵢ⁽⁰⁾ + fⱼ;  \\n    end  \\n  end  \\n  record the total frequency within the current window, fᵢ⁽⁰⁾ along with the associated i-th value in the csv file;  \\nend  \\nreturn csv file ;  \\n\\n![Graph illustrating the sum of frequencies for x-values found at for each window frame](image_chart.png)\\n\\nFigure 16: Graph showing the results of the windowing technique to Figure 15(a)\\n\\nIn the system. This means that the lines found within the knee, foot and flesh regions are automatically classified as non-fractured lines. A total of 16,515 lines extracted from 23 images are employed to evaluate the ADPO scheme. Of the 16,515 lines, 8,035 are fractured lines and 8,480 are non-fractured lines.  \\n\\nThe image evaluation accuracy results for the ADPO line-based fracture detection scheme is presented in Table 5. The average number of lines per image used for the ADPO scheme is 849 lines per image. The accuracy of the system\\n\\n---\\n\\n20ranges from 63.61% to 80.88%. Thus yielding an average accuracy of 72.89%, which is a slight improvement from the average accuracy obtained from the Standard scheme, which is 71.57%. Figure 19 illustrates the performance of the ANN. A total of 11,195 lines are used to test each case for the ANN, whilst 8,035 lines are fractured and 3,160 are non-fractured. The results shows that the accuracy of the ANN requires a minimum of 300 lines to obtain a 65% accuracy. The accuracy achieved for the ANN evaluation is less than the accuracy of the Standard scheme, which obtained a maximum accuracy of 72%. However, the ANN of the ADPO scheme focuses only on the lines in leg-bone region, whereas the ANN of the Standard scheme is exposed to all lines detected in the X-ray image.\\n\\nTable 5: The results for the system’s minimum, average and maximum accuracies for 20 cases over 10 simulations for the ADPO line-based fracture detection scheme.\\n\\n| No. Trained Images | Min Accuracy (%) | Average Accuracy (%) | Max Accuracy (%) |\\n|--------------------|------------------|----------------------|------------------|\\n| 1                  | 65.129           | 73.8656              | 80.866           |\\n| 2                  | 63.294           | 74.7085              | 82.894           |\\n| 3                  | 62.246           | 72.0613              | 81.732           |\\n| 4                  | 63.53            | 72.3312              | 80.751           |\\n| 5                  | 61.629           | 74.2942              | 81.483           |\\n| 6                  | 61.32            | 70.5336              | 80.715           |\\n| 7                  | 65.389           | 74.9762              | 81.163           |\\n| 8                  | 63.76            | 70.1379              | 81.999           |\\n| 9                  | 65.722           | 75.3762              | 81.417           |\\n| 10                 | 62.168           | 70.9889              | 80.648           |\\n| 11                 | 62.313           | 71.7632              | 81.217           |\\n| 12                 | 61.508           | 72.1551              | 80.587           |\\n| 13                 | 65.752           | 74.2622              | 82.955           |\\n| 14                 | 64.087           | 71.1565              | 78.401           |\\n| 15                 | 59.891           | 72.1785              | 80.793           |\\n| 16                 | 65.934           | 71.1875              | 77.312           |\\n| 17                 | 60.539           | 73.1501              | 81.532           |\\n| 18                 | 68.314           | 75.1457              | 81.695           |\\n| 19                 | 64.693           | 72.3942              | 80.339           |\\n| 20                 | 65.05            | 74.5831              | 80.793           |\\n\\n21\\nA PREPRINT - FEBRUARY 21, 2019\\n\\nGraph showing the average accuracy results for the ADPO line-based fracture detection scheme\\n\\n![Graph showing average accuracy results](image-url)\\n\\nNo. Image Trained\\n\\nFigure 17: Graph illustrating the average accuracy for 20 cases over 10 simulations for the improved ADPO line-based fracture detection scheme\\n\\nThe ROC curve for the system evaluation is presented in Figure 18. There are three distinct ROC results illustrated: measured results, estimated ROC, and ROC reference. The measured results are obtained from each case used to evaluate the system. Since the measured results are scattered, a ROC approximation using the measured results is constructed. The estimated ROC curve is employed for the calculation of the AUC, in which the AUC has a value of 0.8271. The ROC reference provides further context of the system\\'s performance relative to a system that randomises binary classification. The AUC value obtained shows that there is a slight improvement from the Standard scheme, where the AUC value is 0.8149. Therefore, the ADPO scheme is more sensitive in detecting true positives than the Standard scheme.\\n\\n22\\n![ROC curve illustrating the TPR and FPR for the ADPO line-based fracture detection for system evaluation](figure18.png)\\n\\nFigure 18: Figure illustrating ROC curve for the ADPO line-based fracture detection\\n\\nGraph illustrating the performance of the ANN for the ADPO line-based fracture detection scheme\\n\\n![Graph showing the system\\'s minimum, average and maximum accuracies of the ADPO line-based fracture detection scheme](figure19.png)\\n\\nFigure 19: Graph showing the system\\'s minimum, average and maximum accuracies of the ADPO line-based fracture detection scheme\\n\\n23\\nA PREPRINT - FEBRUARY 21, 2019\\n\\n5 Critical Analysis\\n\\nThe performance of the ANN is evaluated based on its ability to accurately classify fractured and non-fractured lines. There are four variables that are employed to evaluate the results of both the ANN and the entirety of the system: true positive, true negative, false positive and false negative.\\n\\nIn the medical field, false positives are tolerated, as it is indicative of a cautious system. However, false negatives cannot be tolerated as it means that a fracture is being detected as normal. It is difficult to avoid false negatives when working with ANN\\'s due to its purpose for forming a generalised operation for all input data. Therefore, the number of false negatives must be as minimal as possible. The accuracy, 𝑎, for the evaluation of both the ANN and the system is expressed in (24).\\n\\n\\\\[ a = \\\\frac{\\\\text{TP} + \\\\text{TN}}{\\\\text{TP} + \\\\text{TN} + \\\\text{FP} + \\\\text{FN}} \\\\]\\n\\n(24)\\n\\nFurther evaluation of the ROC curves for both Standard and ADPO schemes in Figure 20; the ROC curve of the ADPO scheme increases in TPR at a lower FPR value compared to the ROC curve of the Standard scheme. The ROC graph of Figure 10 indicates that the scheme reaches maximum sensitivity at 0.4 FPR for the measured results, whilst in Figure 18 the maximum sensitivity is reached at 0.37 FPR. This indicates that the sensitivity performance of the ADPO scheme is slightly better than the Standard scheme.\\n\\n![ROC Curve](image-link)\\n\\nFigure 20: ROC curves for both the Standard and ADPO line-based fracture detection scheme\\n\\nAlthough, the ADPO scheme has a better sensitivity and accuracy performance compared to the Standard scheme, the system’s maximum achievable accuracy is 82.9%, which can be further improved. However, the accuracy of the described schemes are hindered by the labelling of the lines. The approach utilised for the labelling of the fractured and non-fractured lines is an area selection approach, whereby the lines that are within the selected area are labelled as fractures. Lines that are not considered as fractures by human visualisation but are within the selected area are mislabelled. This affects the ANN performance as the weights in the ANN are adjusted due to the mislabelled lines. However, the affect on the weights is minimal as there is only a minority of mislabelled lines. Additionally, the number of features extracted from the lines is limited, because only two points are provided by the Probabilistic Hough\\n\\n24\\nTransform to describe the line detected from the image edge objects of the X-ray image. Moreover, the line detection within the X-ray image is only an approximation of the image edge objects in the X-ray image. An improved detection methodology built from the foundations of the line-based fracture detection scheme, whereby contours are utilised rather for feature extraction instead of lines.\\n\\n## 6  Future Improvements\\n\\nFuture improvements can be implemented to the labelling process of fractured and non-fractured lines, as the current labelling process utilises an area selection approach which mislabels non-fractured lines as fractured lines when the lines are in the area of selection. The improvement to the labelling process includes a deselection process. Thus allowing the user to deselect individual lines from the selected group of fractured lines. This improvement incorporates both area selection and individual deselection. Other improvements is the use of contours over lines, as the detected lines are limited in the number of features that are extracted. Additionally, the lines are an approximation of the edge image objects, whereas contours are a more detailed representation of the edge image objects and holds more points, which result in additional features that can be extracted.\\n\\n## 7  Conclusion\\n\\nTo conclude, this paper details two line-based fracture detection schemes, Standard and ADPO. Both line-based fracture detection schemes utilises 13 extracted features from the detected lines. The difference is that the ADPO scheme optimises the parameters in the Probabilistic Hough Transform for line detection. The optimisation detects more detailed lines for the fractured region within the X-ray image compared to the Standard scheme. Additionally, the ADPO scheme employs a technique which isolates the bones within the leg region from the surrounding lines of the flesh. Therefore, the ANN is only trained with lines within the leg-bone regions. Thus, shifting the focus of the ANN to detect fractures within the leg-bone area only. The system eliminates all other detected lines, which are automatically classified as non-fractures. This includes lines within the knee, foot and surrounding flesh region. In focusing the ANN for lines only in the leg-bone region, it reduces the training complexity of the ANN. The accuracy of the ADPO scheme is slightly better than the accuracy of the Standard scheme. The average accuracy of the ADPO system is 72.89%, whilst the average accuracy of the Standard scheme is 71.57%.\\n\\nThe ANN evaluation without image context indicates that the Standard scheme performs better than the ADPO scheme, however the ADPO ANN only focuses on the lines found within the leg-bone region, whereas the Standard scheme’s ANN is given all detected lines for training. Therefore, the accuracy evaluation will be higher for the Standard scheme as there is a larger number of classified true negatives in the accuracy calculation compared to the ADPO scheme. Further analysis is performed on both schemes to evaluate the ratio of false positive to false negative detection. The ADPO scheme has a higher false negative detection compared to the Standard scheme, but the sensitivity of the ADPO scheme is better than the sensitivity of the Standard scheme. This is determined by employing ROC curves and the calculation of the AUC. The AUC of the Standard scheme is 0.8149, whilst the AUC of the ADPO scheme is 0.8271. Therefore, between the two line-based fracture detection schemes, the ADPO scheme is preferred over the Standard scheme.\\n\\nThe limitation of both schemes lies within the detection of the lines, where the detected lines are an approximation of the image edge objects in the X-ray image. Additionally, the two points provided to describe the lines limits the number of features that are extracted for fracture detection. Due to the limitations of line detection and extracted features an alternative approach is considered. The alternative approach employs the use of contours over the use of lines for feature extraction.\\n\\n## Acknowledgements\\n\\nAcknowledgements are extended to the Department of Radiology at No. 85 Hospital, Shanghai, China for the re-usage of the provided X-ray images.\\n\\n## ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
